\begin{tabular}{cccccccccc}
\toprule
Model & Analytical & General & Inorganic & Materials Science & Organic & Physical & Technical & Toxicity/Safety & Overall Accuracy \\
\midrule
Claude-2 & 0.77 & 0.46 & 0.47 & 0.00 & 0.62 & 0.42 & 0.52 & 0.60 & 0.49 \\
Claude-3 (Opus) & 0.77 & 0.73 & 0.76 & 0.50 & 0.62 & 0.62 & 0.74 & 0.65 & 0.67 \\
Claude-3.5 (Sonnet) & 0.86 & 0.69 & 0.68 & 0.50 & 0.79 & \textbf{0.79} & 0.87 & 0.55 & 0.73 \\
Claude-3.5 (Sonnet) React & 0.77 & 0.81 & 0.71 & 0.50 & \textbf{0.88} & \textbf{0.79} & 0.83 & 0.55 & 0.76 \\
Command-R+ & 0.45 & 0.35 & 0.44 & 0.00 & 0.33 & 0.25 & 0.57 & 0.60 & 0.42 \\
Galactica-120b & 0.00 & 0.00 & 0.03 & 0.00 & 0.00 & 0.12 & 0.00 & 0.05 & 0.02 \\
Gemini-Pro & 0.55 & 0.38 & 0.41 & 0.00 & 0.58 & 0.46 & 0.43 & 0.45 & 0.45 \\
Gemma-1.1-7B-it & 0.36 & 0.31 & 0.44 & 0.50 & 0.46 & 0.29 & 0.39 & 0.40 & 0.35 \\
Gemma-1.1-7B-it-T-one & 0.32 & 0.31 & 0.41 & 0.50 & 0.42 & 0.33 & 0.39 & 0.40 & 0.34 \\
Gemma-2-9B-it & 0.45 & 0.54 & 0.47 & 0.50 & 0.54 & 0.58 & 0.61 & 0.50 & 0.49 \\
Gemma-2-9B-it-T-one & 0.45 & 0.50 & 0.44 & 0.50 & 0.58 & 0.54 & 0.52 & 0.45 & 0.46 \\
GPT-3.5 Turbo & 0.55 & 0.35 & 0.44 & 0.50 & 0.58 & 0.50 & 0.43 & 0.55 & 0.44 \\
GPT-4 & 0.77 & 0.69 & 0.59 & \textbf{1.00} & 0.79 & 0.67 & 0.83 & 0.60 & 0.64 \\
GPT-4o & 0.73 & 0.69 & 0.71 & 0.50 & 0.75 & 0.75 & 0.78 & 0.60 & 0.72 \\
GPT-4o React & 0.55 & 0.58 & 0.68 & 0.50 & 0.75 & 0.58 & 0.74 & 0.45 & 0.62 \\
Human & 0.31 & 0.41 & 0.30 & 0.24 & 0.36 & 0.26 & 0.20 & 0.22 & 0.27 \\
Llama-2-70B Chat & 0.00 & 0.19 & 0.15 & 0.50 & 0.21 & 0.17 & 0.13 & 0.25 & 0.14 \\
Llama-3-8B-Instruct & 0.68 & 0.27 & 0.44 & 0.00 & 0.50 & 0.29 & 0.61 & 0.50 & 0.44 \\
Llama-3-8B-Instruct-T-one & 0.64 & 0.27 & 0.53 & 0.00 & 0.46 & 0.25 & 0.65 & 0.50 & 0.45 \\
Llama-3-70B-Instruct & 0.64 & 0.62 & 0.62 & 0.50 & 0.75 & 0.58 & 0.74 & 0.50 & 0.60 \\
Llama-3-70B-Instruct-T-one & 0.59 & 0.62 & 0.62 & 0.50 & 0.71 & 0.58 & 0.70 & 0.60 & 0.59 \\
Llama-3.1-8B-Instruct & 0.59 & 0.35 & 0.53 & 0.00 & 0.50 & 0.50 & 0.52 & 0.60 & 0.46 \\
Llama-3.1-8B-Instruct-T-one & 0.59 & 0.38 & 0.47 & 0.50 & 0.50 & 0.42 & 0.39 & 0.55 & 0.44 \\
Llama-3.1-70B-Instruct & 0.59 & 0.62 & 0.74 & 0.50 & 0.75 & 0.71 & 0.70 & 0.60 & 0.64 \\
Llama-3.1-70B-Instruct-T-one & 0.45 & 0.58 & 0.59 & 0.50 & 0.67 & 0.62 & 0.65 & 0.45 & 0.57 \\
Llama-3.1-405B-Instruct & 0.68 & 0.73 & 0.74 & \textbf{1.00} & 0.71 & 0.75 & 0.70 & 0.60 & 0.69 \\
Mistral-Large-2 & 0.82 & 0.65 & 0.68 & \textbf{1.00} & 0.75 & 0.71 & 0.70 & 0.50 & 0.68 \\
Mixtral-8x7b-Instruct & 0.41 & 0.35 & 0.47 & 0.50 & 0.33 & 0.33 & 0.30 & 0.40 & 0.36 \\
Mixtral-8x7b-Instruct-T-one & 0.45 & 0.42 & 0.41 & 0.50 & 0.38 & 0.38 & 0.30 & 0.35 & 0.36 \\
o1 & \textbf{0.91} & \textbf{0.92} & \textbf{0.79} & \textbf{1.00} & 0.79 & \textbf{0.79} & \textbf{0.91} & \textbf{0.70} & \textbf{0.80} \\
PaperQA2 & 0.68 & 0.73 & 0.62 & \textbf{1.00} & 0.58 & 0.71 & 0.78 & 0.65 & 0.67 \\
Phi-3-Medium-4k-Instruct & 0.45 & 0.42 & 0.56 & 0.50 & 0.62 & 0.54 & 0.57 & 0.50 & 0.51 \\
\bottomrule
\end{tabular}
