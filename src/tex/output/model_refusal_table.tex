\begin{tabular}{lcccc}
\toprule
\multirow{3}{*}{Model} & \multicolumn{2}{c}{\textbf{Refusal}} & \multicolumn{2}{c}{\textbf{LLM Extraction}}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}\\
& Nº of Questions & Fraction & Nº of Questions & Fraction\\
\midrule
Claude 2 & 100 & 0.035039 & 0 & 0.000000 \\
Claude 3 & 9 & 0.003153 & 48 & 0.016819 \\
Claude 3.5 Sonnet & 33 & 0.011563 & 16 & 0.005606 \\
Command R+ & 0 & 0.000000 & 5 & 0.001752 \\
Gemini Pro & 0 & 0.000000 & 4 & 0.001402 \\
Gemma 1.1 7B & 0 & 0.000000 & 24 & 0.008409 \\
Gemma 1.1 7B Temp=1 & 1 & 0.000350 & 25 & 0.008760 \\
Gemma 2 9B & 5 & 0.001752 & 26 & 0.009110 \\
Gemma 2 9B Temp=1 & 5 & 0.001752 & 37 & 0.012964 \\
GPT-3.5 Turbo & 0 & 0.000000 & 8 & 0.002803 \\
GPT-4 & 0 & 0.000000 & 3 & 0.001051 \\
GPT-4o & 0 & 0.000000 & 7 & 0.002453 \\
Llama 2 70B Chat & 1 & 0.000350 & 3 & 0.001051 \\
Llama 3 70B & 0 & 0.000000 & 16 & 0.005606 \\
Llama 3 70B Temp=1 & 1 & 0.000350 & 17 & 0.005957 \\
Llama 3 8B & 0 & 0.000000 & 9 & 0.003153 \\
Llama 3 8B Temp=1 & 6 & 0.002102 & 11 & 0.003854 \\
Llama 3.1 405B & 0 & 0.000000 & 22 & 0.007708 \\
Llama 3.1 70B & 0 & 0.000000 & 56 & 0.019622 \\
Llama 3.1 70B Temp=1 & 1 & 0.000350 & 252 & 0.088297 \\
Llama 3.1 8B & 0 & 0.000000 & 41 & 0.014366 \\
Llama 3.1 8B Temp=1 & 3 & 0.001051 & 30 & 0.010512 \\
Mistral Large 2 123B & 0 & 0.000000 & 2 & 0.000701 \\
Mixtral 8x7B & 5 & 0.001752 & 59 & 0.020673 \\
Mixtral 8x7B Temp=1 & 8 & 0.002803 & 61 & 0.021374 \\
o1 & 0 & 0.000000 & 2 & 0.000701 \\
Paper QA & 36 & 0.012614 & 53 & 0.018570 \\
Phi 3 Medium 4K & 0 & 0.000000 & 7 & 0.002453 \\
\bottomrule
\end{tabular}
