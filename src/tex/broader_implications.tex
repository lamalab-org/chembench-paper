\paragraph*{Broader implications}

Benchmarking a \glspl{llm}s is not trivial due to many factors.
What does one really investigate in these benchmarks? And what would the ultimate goal be?
If we focus on automated machine-machine interaction where there is a closed-feedback loop,
then focusing on the appropriate prompts and answer quality is a good start. However, if we focus on
the human-machine interaction, we should investigate the simplest prompts, mimicking a real-life use case
for LLMs (e.g. safety profile of a specific molecule in the lab).

Typically, chemists specialize in 1-3 fields, and any human lecturer would be preferred to a \glspl{llm}
at the current stage. However, \glspl{llm} can be used to assist in the teaching process, since its general 
knowledge is vast and sufficient for trivial questions.