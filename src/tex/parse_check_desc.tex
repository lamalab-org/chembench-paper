\paragraph*{Manual validation of parsing performance}

As stated, we tried to account for the variation in outputs with custom regular expressions.
We selected a large, diverse subset of 791 questions (10 questions per topic for all model reports) and manually investigated where the parsed output does not match the actual answer intended by the model.
We found that for \glstext{mcq} questions, the parsing was accurate in 99.76\% of the cases, while for floating point questions, the parsing was accurate in 99.17\% of the cases.
The models generating errors are \texttt{pplx-7b-chat} and \texttt{Mixtral-8x7b}, which in the specified amount of cases fail to follow the prompt.
