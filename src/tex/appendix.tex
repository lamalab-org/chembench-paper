
\subsection{Desired properties of a chemistry benchmark} \label{sec:desired-properties}

\begin{itemize}
    \item \emph{End-to-end automation}. For model development, the evaluations will need to be run many times (e.g., on regular intervals of a training run).
    Approaches that rely on humans scoring the answers of a system\autocite{Schulze_Balhorn_2024, ai4science2023impact, castro2023large} can thus not be used.
    \item \emph{Careful validation by experts}. Manual curation is needed to minimize number of incorrect or unanswerable questions.\autocite{northcutt2021pervasive}
    This is motivated by the observation that many widely used benchmarks are plagued by noisiness.\autocite{Frye_2023, Awg}
    \item \emph{Usable with models that support special treatment of molecules}. Some models such as Galactica\autocite{taylor2022galactica} use special tokenization or encoding procedures for molecules or equations.
    To support this, the benchmark system must encode the semantic meaning of various parts of the question or answer.
    \item \emph{Usable with black box systems}. Many relevant systems do not provide access to model weights or even just the raw logits.
    This might be the case because the systems are proprietary or because they involve not only \glspl{llm} but also external tools such as search \glspl{api} or code executors.\autocite{schick2024toolformer, karpas2022mrkl, yao2022react}
    Thus, a benchmark should not assume access to the raw model outputs but be able to operate on text completions.
    \item \emph{Probing capabilities beyond answering of \glspl{mcq}}. In real world chemistry as well as higher-level university education multiple choice question are seldom utilized.
    Yet, most benchmarking frameworks focus on the \gls{mcq} setting because of the ease of evaluation. Realistic evaluations must measure capabilities beyond the answering of \gls{mcq}.
    \item \emph{Cover a diverse set of topics}. Chemistry, as the \enquote{central science}, bridges multiple disciplines.\autocite{Aspuru_Guzik_2018} To even just approximate \enquote{chemistry capabilities} the topics covered by a chemistry benchmark must be very diverse.
\end{itemize}

\subsection{Related work}
Existing benchmarks such as those from \textcite{guo2023large}, \autocite{sun2023scieval}, \textcite{Schulze_Balhorn_2024}, \textcite{Cai_2024} fail to comply with most of the requirements stipulated above. 
While these benchmarks could provide useful insights in the short-term, they cannot follow the rapid additions to the \gls{llm} space. 
\chembench aims to correct this through a set of developments: compatibility with BigBench, end-to-end automation, special focus on chemical safety, employment of diverse prompting strategies and specialized notation for molecules and mathematical symbols. 
Moreover, we believe that our robust framework, including the platform \url{chembench.org}, will engage the community in open-source contributions.


\subsection{Benchmark corpus}
To ensure maximal interoperability with existing benchmarks or tools, we curated the data in an extended form of the widely used BigBench format.\autocite{srivastava2022beyond}
This also implies that future baselines can be built on top of our infrastructure as long as they are saved in the same format.

\begin{figure}[htb]
    \centering
    \includegraphics{figures/flesch_kincaid_reading_ease.pdf}
    \caption{\textbf{Distribution of Flesch-Kincaid reading ease scores of the questions.} The Flesch-Kincaid reading ease score\autocite{flesch1948new} is a measure of how easy a text is to read. It is calculated based on the average number of syllables per word and the average number of words per sentence. The higher the score, the easier the text is to read. The distribution of the scores of the questions is shown in the histogram. }
    \label{fig:flesch_kincaid_reading_ease}
    \script{wordcloud.py}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics{figures/question_count_barplot_mcq_vs_general.pdf}
    \caption{\textbf{Number of multiple choice questions vs.\ open-ended questions per topic.} The bar plot shows the number of multiple choice questions and general questions per topic.}
    \label{fig:question_count_barplot_mcq_vs_general}
    \script{plot_statistics.py}
\end{figure}

% \subsection{Parsing verification}\label{sec:manually-verified-parsing}
% For validating the parsing workflow, we randomly sampled four questions per topic and manually verified that the completions of the model were parsed correctly.


\subsection{Model performance}
We also evaluated the model performance on the entire \chembench corpus. 
\Cref{fig:barplot_all_correct_all_questions} shows the fraction of questions that were answered completely correctly by the models. 

\begin{figure}[htb]
    \centering
    \includegraphics{figures/overall_performance.pdf}
    \caption{\textbf{Overall performance of the models on the \chembench corpus.} The bar plot shows the fraction of questions that were answered completely correctly by the models. Scores computed on the entire \chembench corpus.}
    \label{fig:barplot_all_correct_all_questions}
    \script{plot_overview_performance_plot.py}
\end{figure}

\Cref{fig:all_questions_models_completely_correct_radar_overall} shows the performance of the models on the different topics of the \chembench corpus.

\begin{figure}[htb]
    \centering
    \includegraphics{figures/all_questions_models_completely_correct_radar_overall.pdf}
    \caption{\textbf{Performance of the models on the different topics of the \chembench corpus.} The radar plot shows the performance of the models on the different topics of the \chembench corpus. The performance is measured as the fraction of questions that were answered completely correctly by the models.
    A score of 1 indicates that all questions were answered completely correctly, while a score of 0 indicates that none of the questions were answered completely correctly.
    }
    \label{fig:all_questions_models_completely_correct_radar_overall}
    \script{analyze_model_reports.py}
\end{figure}


\begin{figure}[htb]
    \centering
    \includegraphics{figures/performance_per_topic.pdf}
    \caption{\textbf{Fraction of completely correctly answered questions per data source.} The heatmap shows, in color, the fraction of questions that were answered completely correctly by different systems for some of our data sources. The performance is measured as the fraction of questions that were answered completely correctly by the models. A score of one (red) indicates that all questions were answered completely correctly, while a score of zero (blue) indicates that none of the questions were answered completely correctly.
        We see that the performance of the models varies significantly between the different data sources. For instance, it is interesting to observe that questions sourced based on textbooks seem easier for our leading models than for humans. However, this performance does not correlate with performance on other sources, e.g., semi-programmatically created tasks such as questions about the number of peaks in an \gls{nmr} spectrum.
    }
    \label{fig:performance_per_topic}
    \script{analyze_performance_per_source.py}
\end{figure}


\begin{figure}[htb]
    \centering
    \includegraphics{figures/parallel_coordinates_overall.pdf}
    \caption{\textbf{Performance of the models on the different topics of the \chembench corpus.} The parallel coordinates plot shows the performance of the models on the different topics of the \chembench corpus. The performance is measured as the fraction of questions that were answered completely correctly by the models. }
    \label{fig:parallel_coordinates_overall}
    \script{make_parallel_coordinates_plot.py}
\end{figure}


\begin{figure}[htb]
    \centering
    \includegraphics{figures/performance_per_topic_tiny.pdf}
    \caption{\textbf{Fraction of completely correctly answered questions per data source on the \enquote{tiny} subset.} The heatmap shows, in color, the fraction of questions that were answered completely correctly by different systems for some of our data sources. The performance is measured as the fraction of questions that were answered completely correctly by the models. A score of one (red) indicates that all questions were answered completely correctly, while a score of zero (blue) indicates that none of the questions were answered completely correctly.
        We see that the performance of the models varies significantly between the different data sources. For instance, it is interesting to observe that questions sourced based on textbooks seem easier for our leading models than for humans. However, this performance does not correlate with performance on other sources, e.g., semi-programmatically created tasks such as questions about the number of peaks in an \gls{nmr} spectrum.
    }
    \label{fig:performance_per_topic_tiny}
    \script{analyze_performance_per_source.py}
\end{figure}



\begin{figure}
    \centering
    \hspace*{-1cm}
    \includegraphics{figures/reading_ease_vs_model_performance.pdf}
    \caption{\textbf{Model performance as a function of reading ease.} The violinplots show the distribution of reading ease scores for questions that were answered completely correctly and those that were not. We do not observe a clear correlation between the reading ease of the questions and the performance of the models. }
    \script{reading_ease_vs_model_performance.py}
    \label{fig:reading_ease_vs_model_performance}
\end{figure}

\subsection{Human baseline}
\paragraph{App} To facilitate the collection of responses, we developed a responsive web application in Typescript using the Next.js\autocite{nextjs} app router framework.
This application handles serving the user interface as well as exposes various \gls{rest} \glspl{api} for relevant operations.
We utilize a MySQL\autocite{mysql} database and Prisma \gls{orm}\autocite{prisma} for efficient database management.
The web application is styled with Tailwind CSS\autocite{tailwindcss} using the shadcn/ui component library and uses NextAuth\autocite{nextauth} for easy and secure user authentication and postMark for sending Emails.
The application is hosted on the Vercel web hosting platform.



\begin{figure}[htb]
    \centering
    \includegraphics{figures/human_timing.pdf}
    \script{analyze_human_data.py}
    \label{fig:human_timing}
    \caption{\textbf{Time taken by human scorers to answer questions vs.\ correctness of their answers.} From the plot it is clear that there is no clear dependence of the correctness of the answers on the time taken by the human scorers to answer the questions.}
\end{figure}


Interestingly, we found no significant correlation between the experience of the human scorers and the performance on the questions (Spearman's \(\rho \approx \variable{output/spearman_experience_score.txt}\), \(p \approx \variable{output/spearman_experience_score_p.txt}\)).

\begin{figure}[htb]
    \centering
    \includegraphics{figures/experience_vs_correctness.pdf}
    \script{analyze_human_data.py}
    \label{fig:experience_vs_correctness}
    \caption{\textbf{Experience of human  scorers vs.\ correctness of their answers.} The experience (in number of years since the first university-level chemistry course) of the human scorers was not significantly correlated with the correctness of their answers.}
\end{figure}


\subsection{Confidence estimates} \label{sec:confidence_estimates}

Since it is important to understand if models can provide an indication of whether their answer might likely be incorrect, we prompted some of our top performing \glspl{llm} to return the confidence in providing a correct answer on an ordinal scale. 

\Cref{fig:confidence_score_distributions} plots the distribution of those scores.

\begin{figure}[htb] 
    \centering
    \includegraphics{figures/confidence_score_distributions.pdf}
    \caption{\textbf{Distribution of confidence scores reported by \glspl{llm}.} \Glspl{llm} show different distributions of confidence scores. The confidence scores are reported on an ordinal scale from 1 to 5, with 1 indicating low confidence and 5 indicating high confidence. The barplots show how many questions were answered with each confidence score.}
    \label{fig:confidence_score_distributions}
    \script{plot_confidence_score_distributions.py}
\end{figure}


\clearpage

\printnoidxglossary[type=\acronymtype, nonumberlist]  % https://github.com/tectonic-typesetting/tectonic/issues/704