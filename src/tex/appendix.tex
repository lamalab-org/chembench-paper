
\subsection{Desired properties of a chemistry benchmark} \label{sec:desired-properties}

\begin{itemize}
    \item \emph{End-to-end automation}. For model development, the evaluations will need to be run many times (e.g., on regular intervals of a training run).
    Approaches that rely on humans scoring the answers of a system\cite{Schulze_Balhorn_2024, ai4science2023impact} can thus not be used.
    \item \emph{Careful validation by experts}. Manual curation is needed to minimize number of incorrect or unanswerable questions.\cite{northcutt2021pervasive}
    This is motivated by the observation that many widely used benchmarks are plagued by noisiness.\cite{Frye_2023, Awg}
    \item \emph{Usable with models that support special treatment of molecules}. Some models such as Galactica\cite{taylor2022galactica} use special tokenization or encoding procedures for molecules or equations.
    To support this, the benchmark system must encode the semantic meaning of various parts of the question or answer.
    \item \emph{Usable with black box systems}. Many relevant systems do not provide access to model weights or even just the raw logits.
    This might be the case because the systems are proprietary or because they involve not only \glspl{llm} but also external tools such as search \glspl{api} or code executors.\cite{schick2024toolformer, karpas2022mrkl, yao2022react}
    Thus, a benchmark should not assume access to the raw model outputs but be able to operate on text completions.
    \item \emph{Probing capabilities beyond answering of \glspl{mcq}}. In real world chemistry as well as higher-level university education multiple choice question are seldom utilized.
    Yet, most benchmarking frameworks focus on the \gls{mcq} setting because of the ease of evaluation. Realistic evaluations must measure capabilities beyond the answering of \gls{mcq}.
    \item \emph{Cover a diverse set of topics}. Chemistry, as the \enquote{central science}, bridges multiple disciplines.\cite{Aspuru_Guzik_2018} To even just approximate \enquote{chemistry capabilities} the topics covered by a chemistry benchmark must be very diverse.
\end{itemize}

\subsection{Benchmark corpus}
To ensure maximal interoperability with existing benchmarks or tools, we curated the data in an extended form of the widely used BigBench format.\cite{srivastava2022beyond}
This also implies that future baselines can be built on top of our infrastructure as long as they are saved in the same format.

\begin{figure}
    \centering
    \includegraphics{figures/flesch_kincaid_reading_ease.pdf}
    \caption{Example question.}
    \label{fig:flesch_kincaid_reading_ease}
\end{figure}

\subsection{Parsing verification}\label{sec:manually-verified-parsing}
For validating the parsing workflow, we randomly sampled four questions per topic and manually verified that the completions of the model were parsed correctly.


\subsection{Model performance}

\begin{figure}
    \centering
    \includegraphics{figures/all_questions_models_completely_correct_radar.pdf}
    \caption{Caption}
    \label{fig:all_questions_models_completely_correct_radar}
    \script{analyze_model_reports.py}
\end{figure}

\subsection{Human baseline}

\begin{figure}
    \centering
    \includegraphics{figures/human_timing.pdf}
    \script{analyze_human_data.py}
    \label{fig:human_timing}
    \caption{}
\end{figure}


Interestingly, we found no significant correlation between the experience of the human scorers and the performance on the questions (Spearman's \(\rho \approx \variable{output/spearman_experience_score.txt}\), \(p \approx \variable{output/spearman_experience_score_p.txt}\)).

\begin{figure}
    \centering
    \includegraphics{figures/experience_vs_correctness.pdf}
    \script{analyze_human_data.py}
    \label{fig:experience_vs_correctness}
    \caption{}
\end{figure}
