\documentclass[11pt, oneside]{article}

\input{preamble.tex}
\usepackage{credits}
\usepackage{orcidlink}
\usepackage{showyourwork}


\title{\textsf{Are frontier models superhuman chemists?}}

\input{authors.tex}
\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\clearpage

\section{Introduction}
\Glspl{llm} are frontier \gls{ml} models trained on massive amounts of text to complete sentences.
While some see in them \enquote{sparks of \gls{agi}},\cite{bubeck2023sparks} others consider them as \enquote{stochasitc parrots}---i.e., systems that only regurgitate what they have been trained on.\cite{bender2021dangers}

Chemists and materials scientists have quickly caught on the mounting attention given to \glspl{llm}, some even suggesting that \enquote{the future of chemistry is language}.\cite{White_2023}
This statement is motivated by a growing number of reports that use \glspl{llm} to properties of molecules or materials,\cite{jablonka202314, jablonka2024leveraging, xie2024fine} to optimize reactions\cite{ramos2023bayesian, kristiadi2024sober} or generate materials,\cite{rubungo2023llm, flam2023language, gruver2024fine} extract information,\cite{Patiny_2023, Dagdelen_2024, Zheng_2024, lála2023paperqa, caufield2023structured} or to even build \enquote{autonomous} systems that can physically perform reactions provided a command in natural language.\cite{bran2023chemcrow, Boiko_2023, darvish2024organa}
The rapid increase in capabilities led to concerns about the potential for dual use of these technologies, e.g., for the design of chemical weapons.\cite{gopal2023releasing, ganguli2022red, Urbina_2022}
Moreover, the use of these models is now also widespread among students\cite{Intelligent.com_2023} as well as research groups. For some users, misleading information---especially about safety-related aspects---might lead to harmful outcomes. Unfortunately, apart from anecdotal reports there is little evidence on how \glspl{llm} perform compared to experts.

Thus, to better understand what \glspl{llm} can do for chemistry and materials science, and where they might be improved with further developments, a comprehensive analysis is needed.
For the development of \glspl{llm}, such evaluation is currently mostly performed via standardized benchmarks such as BigBench\cite{srivastava2022beyond} or the LM Eval Harness.\cite{eval-harness}
The former contains, among 204 tasks, only two tasks classified as \enquote{chemistry related} whereas the latter contains no specific chemistry tasks.
Due to the lack of widely excepted standard benchmarks, the developers of chemical language models\cite{jablonka2024leveraging, guo2023large, ahmad2022chemberta2, Cai_2024} frequently utilize language-interfaced\cite{dinh2022lift} tabular datasets such as the ones reported in MoleculeNet,\cite{wu2018moleculenet} Therapeutic Data Commons\cite{huang2021therapeutics} or MatBench.\cite{dunn2020benchmarking}
While those evaluations can measure how well models can make predictions for very specific tasks, they only give a poor measure of how useful those models might be as a chemical assistant.

While some benchmark based on university entrance exams\cite{Zaki_2024, arora2023llms} or automatic text mining\cite{song2023honeybee, wei2021chemistryqa} have been proposed, also those do not satisfy the following basic criteria chemistry benchmarks should satisfy:
\begin{itemize}
    \item \emph{End-to-end automation}. For model development, the evaluations will need to be run many times (e.g., on regular intervals of a training run).
    Approaches that rely on humans scoring the answers of a system\cite{Schulze_Balhorn_2024, ai4science2023impact} can thus not be used.
    \item \emph{Careful validation by experts}. Manual curation is needed to minimize number of incorrect or unanswerable questions.\cite{northcutt2021pervasive}
    This is motivated by the observation that many widely used benchmarks are plagued by noisiness.\cite{Frye_2023, Awg}
    \item \emph{Usable with models that support special treatment of molecules}. Some models such as Galactica\cite{taylor2022galactica} use special tokenization or encoding procedures for molecules or equations.
    To support this, the benchmark system must encode the semantic meaning of various parts of the question or answer.
    \item \emph{Usable with black box systems}. Many relevant systems do not provide access to model weights or even just the raw logits.
    This might be the case because the systems are proprietary or because they involve not only \glspl{llm} but also external tools such as search \glspl{api} or code executors.\cite{schick2024toolformer, karpas2022mrkl, yao2022react}
    Thus, a benchmark should not assume access to the raw model outputs but be able to operate on text completions.
    \item \emph{Probing capabilities beyond answering of \glspl{mcq}}. In real world chemistry as well as higher-level university education multiple choice question are seldom utilized.
    Yet, most benchmarking frameworks focus on the \gls{mcq} setting because of the ease of evaluation. Realistic evaluations must measure capabilities beyond the answering of \gls{mcq}.
    \item \emph{Cover a diverse set of topics}. Chemistry, as the \enquote{central science}, bridges multiple disciplines.\cite{Aspuru_Guzik_2018} To even just approximate \enquote{chemistry capabilities} the topics covered by a chemistry benchmark must be very diverse.
\end{itemize}


In this work, we report a novel benchmarking framework, chembench, and use it to reveal limitations of current frontier models for the use in the chemical sciences.
Our benchmark consists of \variable{output/total_number_of_questions.txt}\unskip question answer pairs manually (\variable{output/manually_generated.txt}\unskip) or semi-automatically (\variable{output/automatically_generated.txt}\unskip) compiled from diverse sources.
It covers a large fraction of the topics taught in undergraduate chemistry curricula at various skill levels and can be used with any system that can return text (i.e., also tool-augmented systems).

To contextualize the scores, we also surveyed more than \variable{output/number_experts.txt} experts in chemistry (for a total of number of more than \variable{output/total_hours.txt}\unskip) on a subset of the benchmark corpus to be able to compare the performance of current frontier models with the one of humans.
Our results indicate that current frontier models perform \enquote{superhuman} on some aspects of chemistry but in many cases, included safety-related ones, might be very misleading.


\section{Results}

\subsection{Benchmark dataset}

To compile our benchmark corpus we utilized a broad list of sources (\Cref{sec:curation}), ranging from university exams to semi-automatically generated questions based on curated subsets of data in chemical databases.
To ensure maximal interoperability, we curated the data in an extended form of the BigBench format.
This also implies that future baselines can be built on top of our infrastructure as long as they are saved in the same format.
For quality assurance, all questions have been reviewed by at least one scientist in addition to the original curator.

Importantly, our large pool of questions encompassed a wide range of topics.
This can be seen, for example, in \Cref{fig:topic_barplot} in which we compare the number of questions in different categories (see \Cref{sec:meth-topic} for details on how we assigned topics).
By design, a focus of our corpus is on safety-related aspects with a (currently) limited sampling of questions from physical or theoretical chemistry, which might be extended in future work.

\begin{figure}
    \centering
    \includegraphics{figures/question_count_barplot.pdf}
    \caption{\textbf{Number of questions for different topics.} The topics have been assigned using a combination of a rule-based system (mostly based on the source the question has been sampled from) as well as a classifier operating on a word-embedding of the question. The figure shows that not all aspects of chemistry are equally represented in our corpus. The corpus currently focuses on safety-related aspects.}
    \label{fig:topic_barplot}
    \script{plot_statistics.py}
\end{figure}

Importantly, the corpus samples both \gls{mcq} and open-ended questions in a balanced way (XX \gls{mcq} questions vs. XX open-ended questions).
As one might expect, most questions are difficult to read according to the Flesch–Kincaid readability test (\variable{output/flesch_kincaid_reading_ease.txt}).~\cite{kincaid1975derivation}


\begin{figure}
    \centering
    \includegraphics{figures/wordcloud.pdf}
    \caption{\textbf{Wordcloud.}}
    \label{fig:wordcloud}
    \script{wordcloud.py}
\end{figure}


\subsection{\enquote{Tiny} subset}
It is important to note that for routine evaluations, a smaller subset of the corpus might be more practical.\cite{polo2024tinybenchmarks}
For instance,~\citet{liang2023holistic} report costs of more than 10,000 USD for \gls{api} calls for a single evaluation on the \gls{helm} benchmark.


\subsection{Model evaluation}


\begin{figure}
    \centering
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}


\begin{figure}
    \centering
    \includegraphics{figures/chem-bench-completley-correct-calculation-vs-no-calculation.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}


\section{Conclusions}

\subsection{Future work}
- Advanced prompting techniques such as CoT and test time strategies

- More advanced systems with chemnistry specific tools

- More systematic human baseline: Ask people to come in, do not allow to skip questions

\section{Methods}

\subsection{Curation workflow}\label{sec:curation}
For our dataset we curated questions from existing exams, but also programmatically created new questions.
Questions were added via Pull Requests on our GitHub repository and only merged into the main collection after passing manual review.

To ensure that the questions do not enter a training dataset we use the canary string of the Big Bench project.
This requires that \Gls{llm} developers filter their training dataset for this canary string.

\begin{figure}
    \includegraphics[width = \textwidth]{figures/chem-bench.pdf}
        \caption{\textbf{Data curation workflow}.}
\end{figure}

\paragraph{Manually curated questions}

\subparagraph{Analytical chemistry}
The questions are sourced from master student examination papers at the University of Jena, Germany. These questions focus on the principles and techniques employed in analytical chemistry.

\subparagraph{Combustion engineering}
These questions are derived from master student examination papers at the University of Magdeburg, Germany. They explore topics related to combustion processes and engineering principles.

\subparagraph{Functional materials and nano-materials}
The questions are gathered from a seminar conducted at the University of Jena, delving into topics concerning functional materials and nano-materials, exploring their properties and applications.

\subparagraph{General chemistry}
The questions originate from examinations for grade X at Marist Comprehensive Academy Uturu, Nigeria. They cover fundamental concepts in chemistry suitable for students at the secondary school level.

\subparagraph{International Chemical Olympiad (\gls{ICHO})}
The questions are sourced from various countries' Chemical Olympiads, including those held in the USA, UK, and Moldova, providing challenging problems for competitive chemistry.

\subparagraph{Material synthesis}
These questions are derived from seminars on material synthesis at the University of Jena, focusing on the methods and techniques employed in the synthesis of various materials.

\subparagraph{Organic reactivity}
The questions are primarily sourced from tutorial papers from the Technical University of Munich, Germany, exploring reactions and mechanisms in organic chemistry.

\subparagraph{Periodic table}
The questions are framed based on the reference periodic table from the Royal Society of Chemistry (\url{https://www.rsc.org/periodic-table/}), covering various aspects and trends of the periodic table.

\subparagraph{Polymer chemistry}
Questions are sourced from examinations held at the University of Hamburg, covering topics related to polymer chemistry and its applications.

\subparagraph{Chemical reactivity}
These questions are framed based on information from the Cameo Chemicals website (\url{https://cameochemicals.noaa.gov/reactivity}), exploring the reactivity of various chemicals and compounds.

\subparagraph{Chemical safety}
The questions encompass various aspects of safety, including lab safety, toxicology, material safety data sheets, carcinogens, sourced from reputable safety resources.

\subparagraph{Shape of molecules}
The questions are sourced from examinations held at the Technical University of Munich, Germany, focusing on determining the shape and geometry of molecules.

\subparagraph{\gls{NMR} spectroscopy}
Questions are sourced from the twitter page called (\url{https://twitter.com/NMRspectroscopy}), focusing on concepts and applications of nuclear magnetic resonance spectroscopy.


\paragraph{Semi-programatically generated questions}

\subparagraph{Oxidation states}
The questions regarding oxidation states are extracted from the website \url{https://www.cheminfo.org/}. These exercises focus on determining the number of isomers for specific molecules. To perform these calculations, we utilize the RDKit package \cite{bento2020open}.
\subparagraph{Electron count of atoms}
Questions pertaining to the electron count of atoms are gathered from the website \url{https://www.cheminfo.org/}. These inquiries involve determining the electron count of atoms in various chemical contexts. The calculations for these tasks are facilitated using the RDKit package \cite{bento2020open}.
\subparagraph{Point group for molecules}
Questions involve predicting the point group for molecules. The ChemCaption tool (\url{https://github.com/lamalab-org/chem-caption/tree/main}) is utilized for data generation. Given a molecule, predict its point group.
\subparagraph{SMILES-IUPAC}
Web scrapped SMILES IUPAC pairs from the PubChem database \cite{pubchem} and then framed questions involve converting SMILES notation to IUPAC nomenclature, utilizing information from the PubChem database.

\paragraph{Programmatically generated questions}
\subparagraph{Number of NMR peaks} To generate tasks about the number of \gls{nmr} peaks, we randomly sampled SMILES from the ZINC dataset and then used OpenChemLib\cite{openchemlib} to compute the number of diasterotopically distinct hydrogen atoms.
We then sampled from the set of number of peaks (excluding the correct answer) to obtain the false answer options.

\subparagraph{GHS, hazard statements and DAI}
The \gls{ghs} classification, hazard statements and \gls{dai} data have been extracted from PubChem.~\cite{pubchem}
The former two have been mined via the PubChem \gls{api}, while the latter has been manually compiled.
The \glspl{dai} have been curated to contain only records approved by the \gls{who}.
The chemicals in this class of questions belong to one of the three classes: pesticides (e.g., calcium arsenate), insecticides (e.g., cyfluthrin) or herbicides (e.g. 2,4-D).
All data was saved in tabular form, and then we programmatically created questions (\variable{output/num_h_statements.txt} hazard statement definitions, \variable{output/num_pictograms.txt} chemical-\gls{ghs} pictogram matching, \variable{output/num_dai.txt} \glspl{dai}).


\subparagraph{Number of isomers}
We used MAYGEN\cite{Yirik_2021} to compute the number of isomers for a set of randomly sampled SMILES from the ZINC dataset.
To obtain the answers for false options we sampled from the set of number of isomers.

\subsection{Model evaluation workflow}

\paragraph{Prompting}

To maintain the consistency with the training, we employ distinct prompt templates in general tailored for completion models and instruction-tuned models. 
We pose instruction or impose constraints on the models within these templates to receive response in a specific format so that a robust, fair and consistent parsing can be performed as explained in \Graf{sec:parsing}.
Certain models are trained with special annotations and LaTeX syntax for scientific notations, chemical reactions, or symbols embedded within the text. 
For example, all the SMILES representations are encapsulated within \texttt{[START_SMILES][\text backslash END_SMILES]} in Galactica\cite{taylor2022galactica}.
Our prompting strategy consistently adheres to  these details in a model-specific manner by post-processing \latex syntax, chemical symbols, chemical equations and physical units (by either adding or removing wrappers).

\subparagraph{Completion models}
\texttt{Galactica-120b}\cite{taylor2022galactica}


\subparagraph{Instruction-tuned models} \texttt{Claude 2} \texttt{Claude3 (Opus)}\cite{anthropicClaudeModelFamily2024}
\texttt{GPT-4}\cite{openai2024gpt4}
\texttt{GPT-3.5-turbo}\cite{brown2020language}
\texttt{Gemini Pro}\cite{gemini}
\texttt{Mixtral-8x7b}\cite{jiang2024mixtral}
\texttt{Llama2-70b}\cite{touvron2023llama}
\texttt{pplx-7b-chat}

\paragraph{Parsing}
Our parsing workflow is, by default, multistep and primarily based on regular expressions.
In the case of instruction-tuned models we first attempt to identify the \texttt{[ANSWER][\textbackslash ANSWER]} environment we prompt the model to report the answer in.
In the case of completion models, this step is skipped. From there, we attempt to extract the relevant enumeration letters (for multiple-choice questions) or numbers.
In the case of numbers, our regular expression was engineered to be able to deal with various forms of scientific notation.
As initial tests indicated that models sometimes return integers in the form of words, e.g. \enquote{one} instead of \enquote{1}, we also implemented a word-to-number conversion.
In case these hard-coded parsing steps fail, we fall back to using a \gls{llm}, e.g. Claude-2, to parse the completion.
The frequency of this fallback being triggered was very different for different models (see XX).
Manual verification (see \Cref{sec:manually-verified-parsing}) indicates that this \gls{llm}-based parsing is not a relevant error source.

\paragraph{Models}


\subparagraph{Tool augmented models}
In addition to directly prompting \glspl{llm}, we also investigated the performance of tool-augmented systems.
For this we, on the one hand, investigated the \texttt{online} model of Perplexity.AI and, on the other hand, \texttt{gpt-3.5-turbo} as well as \texttt{claude-2}, with ReAct-style tool augmentation.\cite{yao2023react}
The latter two models had access to WolframAlpha, the ArXiv \gls{api}, a Python interpreter, as well as web search (using DuckDuckGo).
We implemented the system using Langchain with the default prompts and constrained the system to a maximum of ten \gls{llm} calls.


\subsection{Human baseline}

\paragraph{App} To facilitate the collection of responses, we developed a responsive web application in Typescript using the Next.js\cite{nextjs} app router framework.
This application handles serving the user interface as well as exposes various \gls{rest} \glspl{api} for relevant operations.
We utilize a MySQL\cite{mysql} database and Prisma \gls{orm}\cite{prisma} for efficient database management.
The web application is styled with Tailwind CSS\cite{tailwindcss} using the shadcn/ui component library and uses NextAuth\cite{nextauth} for easy and secure user authentication and postMark for sending Emails.
The application is hosted on the Vercel web hosting platform.

\paragraph{Question selection}
Since we anticipated that we will not be able to collect enough responses for every question to allow for a meaningful statistical analysis, we decided on showing a relevant subset of all questions to the human scorers.
For selecting the subset, we decided on addressing two questions:
\begin{itemize}
    \item Are the questions for which the models scored poorly just too difficult or unanswerable?
    \item Are there areas in which the performance of humans is very different from the ones of the models?
\end{itemize}
To answer the first question we selected X questions that all \glspl{llm} (model names) from an initial scoring round did not answer correctly.
From those we picked X diverse one using greedy MaxMin sampling on the embeddings on the questions computed using BART (see below).


\paragraph{Study design}
For our initial study we wanted to maximize the response rate given our available resources. For this reason, we did not opt for a highly controlled study setting. That is, while users were prompted to not use external tools other than a calculator and to not consult with other humans, we do not have any way to verify that the participants complied with those rules. Note that users were also allowed to skip questions.


\subsection{Classification of questions into topics}\label{sec:meth-topic} When curating our dataset we systematically recorded keywords and sources.
To allow for analysis of the model performance as a function of the topic, we leverage this information together with sequence classification models.
For questions which can easily be assigned to a topic based on the source (e.g., number of NMR peaks, chemical compatibility, toxicology exam questions) we use this information to make the assignment.
For the remaining ones, e.g., from chemistry olympiad questions, we use zero-short sequence classification\cite{zeroshotsequence} using the BART model\cite{bart, FacebookBART}, which our preliminary analysis found to be more robust than topic modeling based on embeddings from OpenAI's \texttt{ada} model or Cohere's \texttt{Cohembed-english-v3.0} model.


\section*{Data and code availability}

\section*{Acknowledgements}
This work was supported by the Carl Zeiss Foundation and a \enquote{Talent Fund} of the \enquote{Life} profile line of the Friedrich Schiller University Jena.
We also want to acknowledge access to the HPC cluster of Stability.AI.

\section*{Conflicts of interest}
K.M.J.\ is a paid consultant for OpenAI. M.P.\ is employee of Stability.ai and A.M.\ and N.A.\ are paid contractors of Stability.ai.

\section*{Author contributions}


\bibliography{references}

\appendix

\subsection{Benchmark corpus}

\begin{figure}
    \centering
    \includegraphics{figures/flesch_kincaid_reading_ease.pdf}
    \caption{Example question.}
    \label{fig:flesch_kincaid_reading_ease}
\end{figure}

\subsection{Parsing verification}\label{sec:manually-verified-parsing}
For validating the parsing workflow, we randomly sampled four questions per topic and manually verified that the completions of the model were parsed correctly.


\subsection{Model performance}

\begin{figure}
    \centering
    \includegraphics{figures/all_questions_models_completely_correct_radar.pdf}
    \caption{Caption}
    \label{fig:all_questions_models_completely_correct_radar}
    \script{analyze_model_reports.py}
\end{figure}

\subsection{Human baseline}

\begin{figure}
    \centering
    \includegraphics{figures/human_timing.pdf}
    \script{analyze_human_data.py}
    \label{fig:human_timing}
    \caption{}
\end{figure}


Interestingly, we found no significant correlation between the experience of the human scorers and the performance on the questions (Spearman's \(\rho \approx \variable{output/spearman_\experience_score.txt}\), \(p \approx \variable{output/spearman_\experience_score_p.txt}\)).

\begin{figure}
    \centering
    \includegraphics{figures/experience_vs_correctness.pdf}
    \script{analyze_human_data.py}
    \label{fig:experience_vs_correctness}
    \caption{}
\end{figure}

\end{document}
