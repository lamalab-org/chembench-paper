\documentclass[11pt, oneside]{article}

\input{preamble.tex}
\usepackage{credits}
\usepackage{orcidlink}
\usepackage{showyourwork}


\title{\textsf{Are frontier models superhuman chemists?}}

\input{authors.tex}
\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\clearpage

\section{Introduction}
\Glspl{llm} are frontier \gls{ml} models trained on massive amounts of text to complete sentences.
While some see in them \enquote{sparks of \gls{agi}},\cite{bubeck2023sparks} others consider them as \enquote{stochasitc parrots}---i.e., systems that only regurgitate what they have been trained on.\cite{bender2021dangers}
In any case, the promise of so-called foundation models (the leading ones being called frontier models) is that they can solve a wide variety of tasks they have not been explicitly trained on.\cite{bommasani2021opportunities, anderljung2023frontier}
In addition, there is tremendous economic interest and investment in those generative models is said to lead to a market of more than \$1.3 trillion (almost \$30 billion for drug discovery applications) by 2032.\cite{bloomberg}

Chemists and materials scientists have quickly caught on the mounting attention given to \glspl{llm}, some even suggesting that \enquote{the future of chemistry is language}.\cite{White_2023}
This statement is motivated by a growing number of reports that use \glspl{llm} to properties of molecules or materials,\cite{jablonka202314, jablonka2024leveraging, xie2024fine} to optimize reactions\cite{ramos2023bayesian, kristiadi2024sober} or generate materials,\cite{rubungo2023llm, flam2023language, gruver2024fine} extract information,\cite{Patiny_2023, Dagdelen_2024, Zheng_2024, lála2023paperqa, caufield2023structured} or to even build \enquote{autonomous} systems that can physically perform reactions provided a command in natural language.\cite{bran2023chemcrow, Boiko_2023, darvish2024organa}
But even more than that, since a lot---if not most---of the information about chemistry is currently stored and communicated in text. 
Most insights in chemical research does not directly originate from data stored in databases but rather from the scientists and their ability to interpret data. 
Most of these insights are in form of text in scientific publications. 
Thus, operating on such text might be our best way of \enquote{unlocking} and learning from these insights.
This might ultimately lead to \enquote{copilots} for chemists that can provide answers to questions or even suggest new experiments.
However, the rapid increase in capabilities of chemical \gls{ml} models, led to concerns about the potential for dual use of these technologies, e.g., for the design of chemical weapons.\cite{gopal2023releasing, ganguli2022red, Urbina_2022}
To some extent, this is not surprising as any technology, that, for instance, can predict the toxicity of molecules (to be used in drug discovery) can also be used to predict the toxicity of molecules (to be used in chemical warfare).

It is important to realize that the user base of such models is not only limited to experts in chemistry and materials science. 
For example, many students frequently consolidate those tools---perhaps even to prepare chemical experiments.\cite{Intelligent.com_2023}
This also implies to users from the general public, who might consider using \glspl{llm} to answer questions about the safety of chemicals.
Thus, for some users, misleading information---especially about safety-related aspects---might lead to harmful outcomes. 
Unfortunately, apart from anecdotal reports there is little evidence on how \glspl{llm} perform compared to experts.

Thus, to better understand what \glspl{llm} can do for chemistry and materials science, and where they might be improved with further developments, evaluation frameworks are needed that allow to systematically measure progress and mitigate potential harms.
For the development of \glspl{llm}, such evaluation is currently mostly performed via standardized benchmarks such as BigBench\cite{srivastava2022beyond} or the LM Eval Harness.\cite{eval-harness}
The former contains, among 204 tasks, only two tasks classified as \enquote{chemistry related} whereas the latter contains no specific chemistry tasks.
Due to the lack of widely excepted standard benchmarks, the developers of chemical language models\cite{jablonka2024leveraging, guo2023large, ahmad2022chemberta2, Cai_2024} frequently utilize language-interfaced\cite{dinh2022lift} tabular datasets such as the ones reported in MoleculeNet,\cite{wu2018moleculenet} Therapeutic Data Commons\cite{huang2021therapeutics} or MatBench.\cite{dunn2020benchmarking}
While those evaluations can measure how well models can make predictions for very specific tasks (e.g., predicting solubility for a given molecule reported as \gls{smiles}), they only give a poor measure of how useful those models might be as a chemical assistant.

%ToDo: this is too technical and should be moved to the methods
While some benchmark based on university entrance exams\cite{Zaki_2024, arora2023llms} or automatic text mining\cite{song2023honeybee, wei2021chemistryqa} have been proposed, also those do not satisfy the following basic criteria chemistry benchmarks should satisfy:
\begin{itemize}
    \item \emph{End-to-end automation}. For model development, the evaluations will need to be run many times (e.g., on regular intervals of a training run).
    Approaches that rely on humans scoring the answers of a system\cite{Schulze_Balhorn_2024, ai4science2023impact} can thus not be used.
    \item \emph{Careful validation by experts}. Manual curation is needed to minimize number of incorrect or unanswerable questions.\cite{northcutt2021pervasive}
    This is motivated by the observation that many widely used benchmarks are plagued by noisiness.\cite{Frye_2023, Awg}
    \item \emph{Usable with models that support special treatment of molecules}. Some models such as Galactica\cite{taylor2022galactica} use special tokenization or encoding procedures for molecules or equations.
    To support this, the benchmark system must encode the semantic meaning of various parts of the question or answer.
    \item \emph{Usable with black box systems}. Many relevant systems do not provide access to model weights or even just the raw logits.
    This might be the case because the systems are proprietary or because they involve not only \glspl{llm} but also external tools such as search \glspl{api} or code executors.\cite{schick2024toolformer, karpas2022mrkl, yao2022react}
    Thus, a benchmark should not assume access to the raw model outputs but be able to operate on text completions.
    \item \emph{Probing capabilities beyond answering of \glspl{mcq}}. In real world chemistry as well as higher-level university education multiple choice question are seldom utilized.
    Yet, most benchmarking frameworks focus on the \gls{mcq} setting because of the ease of evaluation. Realistic evaluations must measure capabilities beyond the answering of \gls{mcq}.
    \item \emph{Cover a diverse set of topics}. Chemistry, as the \enquote{central science}, bridges multiple disciplines.\cite{Aspuru_Guzik_2018} To even just approximate \enquote{chemistry capabilities} the topics covered by a chemistry benchmark must be very diverse.
\end{itemize}


In this work, we report a novel benchmarking framework, chembench, and use it to reveal limitations of current frontier models for the use in the chemical sciences.
Our benchmark consists of \variable{output/total_number_of_questions.txt}\unskip question answer pairs manually (\variable{output/manually_generated.txt}\unskip) or semi-automatically (\variable{output/automatically_generated.txt}\unskip) compiled from diverse sources.
It covers a large fraction of the topics taught in undergraduate chemistry curricula at various skill levels and can be used with any system that can return text (i.e., also tool-augmented systems).

To contextualize the scores, we also surveyed more than \variable{output/number_experts.txt} experts in chemistry (for a total of number of more than \variable{output/total_hours.txt}\unskip) on a subset of the benchmark corpus to be able to compare the performance of current frontier models with the one of humans.
Our results indicate that current frontier models perform \enquote{superhuman} on some aspects of chemistry but in many cases, included safety-related ones, might be very misleading.


\section{Results}

\subsection{Benchmark dataset}

To compile our benchmark corpus we utilized a broad list of sources (see \Cref{sec:curation}), ranging from university exams to semi-automatically generated questions based on curated subsets of data in chemical databases.
To ensure maximal interoperability, we curated the data in an extended form of the widely used BigBench format.
This also implies that future baselines can be built on top of our infrastructure as long as they are saved in the same format.


Importantly, our large pool of questions encompassed a wide range of topics.
This can be seen, for example, in \Cref{fig:topic_barplot} in which we compare the number of questions in different categories (see \Cref{sec:meth-topic} for details on how we assigned topics).
By design, a focus of our corpus is on safety-related aspects with a (currently) limited sampling of questions from physical or theoretical chemistry, which might be extended in future work.
For quality assurance, all questions have been reviewed by at least one scientist in addition to the original curator.

\begin{figure}
    \centering
    \includegraphics{figures/question_count_barplot.pdf}
    \caption{\textbf{Number of questions for different topics.} The topics have been assigned using a combination of a rule-based system (mostly based on the source the question has been sampled from) as well as a classifier operating on a word-embedding of the question. The figure shows that not all aspects of chemistry are equally represented in our corpus. The corpus currently focuses on safety-related aspects.}
    \label{fig:topic_barplot}
    \script{plot_statistics.py}
\end{figure}

Importantly, the corpus samples both \gls{mcq} and open-ended questions in a balanced way (\variable{output/mcq_questions.txt} \gls{mcq} questions vs. \variable{output/non_mcq_questions.txt} open-ended questions).
As one might expect, most questions are difficult to read according to the Flesch–Kincaid readability test (\variable{output/flesch_kincaid_reading_ease.txt}).~\cite{kincaid1975derivation}


\begin{figure}
    \centering
    \includegraphics{figures/wordcloud.pdf}
    \caption{\textbf{Wordcloud.}}
    \label{fig:wordcloud}
    \script{wordcloud.py}
\end{figure}


\subsection{\enquote{Tiny} subset}
It is important to note that for routine evaluations, a smaller subset of the corpus might be more practical.\cite{polo2024tinybenchmarks}
For instance,~\citet{liang2023holistic} report costs of more than 10,000 USD for \gls{api} calls for a single evaluation on the \gls{helm} benchmark on a model. 
To address this, we also provide a subset (\variable{output/num_tiny_questions.txt} questions) of the corpus that was curated to be a diverse and representative subset of the full corpus (see \Cref{sec:subset-selection} for details on the curation process).
We also used this subset to seed the app we used for the human baseline study. 


\subsection{Model evaluation}


\begin{figure}
    \centering
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}


\begin{figure}
    \centering
    \includegraphics{figures/chem-bench-completley-correct-calculation-vs-no-calculation.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}


\section{Conclusions}

\subsection{Future work}
- Advanced prompting techniques such as CoT and test time strategies

- More advanced systems with chemnistry specific tools

- More systematic human baseline: Ask people to come in, do not allow to skip questions

\section{Methods}

\subsection{Curation workflow}\label{sec:curation}
For our dataset we curated questions from existing exams, but also programmatically created new questions.
Questions were added via Pull Requests on our GitHub repository and only merged into the main collection after passing manual review.

To ensure that the questions do not enter a training dataset we use the canary string of the Big Bench project.
This requires that \Gls{llm} developers filter their training dataset for this canary string.

\begin{figure}
    \includegraphics[width = \textwidth]{figures/chem-bench.pdf}
        \caption{\textbf{Data curation workflow}.}
\end{figure}

\paragraph{Manually curated questions}

\paragraph{Semi-programatically generated questions}

\subparagraph{Oxidation states}

\subparagraph{Electron counts}

\paragraph{Programmatically generated questions}
\subparagraph{Number of NMR peaks} To generate tasks about the number of \gls{nmr} peaks, we randomly sampled SMILES from the ZINC dataset and then used OpenChemLib\cite{openchemlib} to compute the number of diasterotopically distinct hydrogen atoms.
We then sampled from the set of number of peaks (excluding the correct answer) to obtain the false answer options.

\subparagraph{GHS, hazard statements and DAI}
The \gls{ghs} classification, hazard statements and \gls{dai} data have been extracted from PubChem.~\cite{pubchem}
The former two have been mined via the PubChem \gls{api}, while the latter has been manually compiled.
The \glspl{dai} have been curated to contain only records approved by the \gls{who}.
The chemicals in this class of questions belong to one of the three classes: pesticides (e.g., calcium arsenate), insecticides (e.g., cyfluthrin) or herbicides (e.g. 2,4-D).
All data was saved in tabular form, and then we programmatically created questions (\variable{output/num_h_statements.txt} hazard statement definitions, \variable{output/num_pictograms.txt} chemical-\gls{ghs} pictogram matching, \variable{output/num_dai.txt} \glspl{dai}).


\subparagraph{Number of isomers}
We used MAYGEN\cite{Yirik_2021} to compute the number of isomers for a set of randomly sampled SMILES from the ZINC dataset.
To obtain the answers for false options we sampled from the set of number of isomers.

\subsection{Model evaluation workflow}

\paragraph{Prompting}

To maintain the consistency with the training, we employ distinct prompt templates in general tailored for completion models and instruction-tuned models. 
We pose instruction or impose constraints on the models within these templates to receive response in a specific format so that a robust, fair and consistent parsing can be performed as explained in \Cref{sec:parsing}.
Certain models are trained with special annotations and LaTeX syntax for scientific notations, chemical reactions, or symbols embedded within the text. 
For example, all the SMILES representations are encapsulated within \texttt{[START\_SMILES][\text backslash END\_SMILES]} in Galactica\cite{taylor2022galactica}.
Our prompting strategy consistently adheres to  these details in a model-specific manner by post-processing \LaTeX syntax, chemical symbols, chemical equations and physical units (by either adding or removing wrappers).

\subparagraph{Completion models}
\texttt{Galactica-120b}\cite{taylor2022galactica}


\subparagraph{Instruction-tuned models} \texttt{Claude 2} \texttt{Claude3 (Opus)}\cite{anthropicClaudeModelFamily2024}
\texttt{GPT-4}\cite{openai2024gpt4}
\texttt{GPT-3.5-turbo}\cite{brown2020language}
\texttt{Gemini Pro}\cite{gemini}
\texttt{Mixtral-8x7b}\cite{jiang2024mixtral}
\texttt{Llama2-70b}\cite{touvron2023llama}
\texttt{pplx-7b-chat}

\paragraph{Parsing}
Our parsing workflow is, by default, multistep and primarily based on regular expressions.
In the case of instruction-tuned models we first attempt to identify the \texttt{[ANSWER][\textbackslash ANSWER]} environment we prompt the model to report the answer in.
In the case of completion models, this step is skipped. From there, we attempt to extract the relevant enumeration letters (for multiple-choice questions) or numbers.
In the case of numbers, our regular expression was engineered to be able to deal with various forms of scientific notation.
As initial tests indicated that models sometimes return integers in the form of words, e.g. \enquote{one} instead of \enquote{1}, we also implemented a word-to-number conversion.
In case these hard-coded parsing steps fail, we fall back to using a \gls{llm}, e.g. Claude-2, to parse the completion.
The frequency of this fallback being triggered was very different for different models (see XX).
Manual verification (see \Cref{sec:manually-verified-parsing}) indicates that this \gls{llm}-based parsing is not a relevant error source.

\paragraph{Models}


\subparagraph{Tool augmented models}
In addition to directly prompting \glspl{llm}, we also investigated the performance of tool-augmented systems.
For this we, on the one hand, investigated the \texttt{online} model of Perplexity.AI and, on the other hand, \texttt{gpt-3.5-turbo} as well as \texttt{claude-2}, with ReAct-style tool augmentation.\cite{yao2023react}
The latter two models had access to WolframAlpha, the ArXiv \gls{api}, a Python interpreter, as well as web search (using DuckDuckGo).
We implemented the system using Langchain with the default prompts and constrained the system to a maximum of ten \gls{llm} calls.


\subsection{Human baseline}

\paragraph{App} To facilitate the collection of responses, we developed a responsive web application in Typescript using the Next.js\cite{nextjs} app router framework.
This application handles serving the user interface as well as exposes various \gls{rest} \glspl{api} for relevant operations.
We utilize a MySQL\cite{mysql} database and Prisma \gls{orm}\cite{prisma} for efficient database management.
The web application is styled with Tailwind CSS\cite{tailwindcss} using the shadcn/ui component library and uses NextAuth\cite{nextauth} for easy and secure user authentication and postMark for sending Emails.
The application is hosted on the Vercel web hosting platform.

\paragraph{Question selection} \label{sec:subset-selection}
Since we anticipated that we will not be able to collect enough responses for every question to allow for a meaningful statistical analysis, we decided on showing a relevant subset of all questions to the human scorers.
For selecting the subset, we decided on addressing two questions:
\begin{itemize}
    \item Are the questions for which the models scored poorly just too difficult or unanswerable?
    \item Are there areas in which the performance of humans is very different from the ones of the models?
\end{itemize}
To answer the first question we selected X questions that all \glspl{llm} (model names) from an initial scoring round did not answer correctly.
From those we picked X diverse one using greedy MaxMin sampling on the embeddings on the questions computed using BART (see below).


\paragraph{Study design}
For our initial study we wanted to maximize the response rate given our available resources. For this reason, we did not opt for a highly controlled study setting. That is, while users were prompted to not use external tools other than a calculator and to not consult with other humans, we do not have any way to verify that the participants complied with those rules. Note that users were also allowed to skip questions.


\subsection{Classification of questions into topics}\label{sec:meth-topic} When curating our dataset we systematically recorded keywords and sources.
To allow for analysis of the model performance as a function of the topic, we leverage this information together with sequence classification models.
For questions which can easily be assigned to a topic based on the source (e.g., number of NMR peaks, chemical compatibility, toxicology exam questions) we use this information to make the assignment.
For the remaining ones, e.g., from chemistry olympiad questions, we use zero-short sequence classification\cite{zeroshotsequence} using the BART model\cite{bart, FacebookBART}, which our preliminary analysis found to be more robust than topic modeling based on embeddings from OpenAI's \texttt{ada} model or Cohere's \texttt{Cohembed-english-v3.0} model.


\section*{Data and code availability}
The code and data for chembench is available at \url{https://github.com/lamalab-org/chem-bench} and archived at \url{XXX}.
The code for the app for our human baseline study is available at \url{https://github.com/lamalab-org/chem-bench-app}. 
To ensure reproducibility, this manuscript was generated using the \href{https://show-your.work/en/latest/}{showyourwork!} framework.\cite{Luger2021}
The code to rebuild the paper (including code for all figures and numbers next to which there is a GitHub icon) can be found at \url{https://github.com/lamalab-org/chembench-paper}.

\section*{Acknowledgements}
This work was supported by the Carl Zeiss Foundation and a \enquote{Talent Fund} of the \enquote{Life} profile line of the Friedrich Schiller University Jena.
We also want to acknowledge access to the HPC cluster of Stability.AI.

\section*{Conflicts of interest}
K.M.J.\ is a paid consultant for OpenAI. M.P.\ is employee of Stability.ai and A.M.\ and N.A.\ are paid contractors of Stability.AI.

\section*{Author contributions}

\footnotesize
\insertcredits
\normalsize

\bibliography{references}

\appendix

\subsection{Benchmark corpus}

\begin{figure}
    \centering
    \includegraphics{figures/flesch_kincaid_reading_ease.pdf}
    \caption{Example question.}
    \label{fig:flesch_kincaid_reading_ease}
\end{figure}

\subsection{Parsing verification}\label{sec:manually-verified-parsing}
For validating the parsing workflow, we randomly sampled four questions per topic and manually verified that the completions of the model were parsed correctly.


\subsection{Model performance}

\begin{figure}
    \centering
    \includegraphics{figures/all_questions_models_completely_correct_radar.pdf}
    \caption{Caption}
    \label{fig:all_questions_models_completely_correct_radar}
    \script{analyze_model_reports.py}
\end{figure}

\subsection{Human baseline}

\begin{figure}
    \centering
    \includegraphics{figures/human_timing.pdf}
    \script{analyze_human_data.py}
    \label{fig:human_timing}
    \caption{}
\end{figure}


Interestingly, we found no significant correlation between the experience of the human scorers and the performance on the questions (Spearman's \(\rho \approx \variable{output/spearman_experience_score.txt}\), \(p \approx \variable{output/spearman_experience_score_p.txt}\)).

\begin{figure}
    \centering
    \includegraphics{figures/experience_vs_correctness.pdf}
    \script{analyze_human_data.py}
    \label{fig:experience_vs_correctness}
    \caption{}
\end{figure}

\end{document}
