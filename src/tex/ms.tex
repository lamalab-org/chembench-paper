\documentclass[11pt, oneside]{article}

\input{preamble.tex}
\usepackage{credits}
\usepackage{orcidlink}
\usepackage{showyourwork}


\title{\textsf{Are large language models superhuman chemists?}}

\input{authors.tex}
\begin{document}
\maketitle

% make the app somewhat more prominent 

\clearpage
\begin{abstract}
    Large language models (LLMs) have drawn great interest as they can perform tasks they have not been explicitly been trained on. 
    This is relevant for the chemical sciences, which face the problem of learning from many small and diverse datasets that are frequently in the form of text.
    LLMs are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously.

    However, we still have only very limited systematic understanding of the chemical reasoning capabilities of LLMs, which would be required to improve models and mitigate potential harms. 
    Here, we introduce \enquote{\chembench}, an automated framework designed to rigorously evaluate the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of human chemists.

    We curated more than 6000 question-answer pairs about a wide array of subfields, evaluated leading models such as GPT-4 and Claude 3, and found that the best models, on average, outperform the best human chemists in our study. 
    The models, however, struggle with some chemical reasoning tasks that are easy for human experts and give overconfident misleading predictions, e.g., about the safety profile of chemicals. 

    These findings underscore the dual reality that, although LLMs demonstrate remarkable proficiency in chemical tasks, there is a critical need for further research to enhance their safety and utility in chemical sciences.
    Our findings also indicate a need for adaptation to chemistry curricula and highlight the importance of continued development of evaluation frameworks for systematic improvement of safe and useful LLMs.
\end{abstract}

\clearpage
\begin{refsection}
\section{Introduction}
\Glspl{llm} are \gls{ml} models trained on massive amounts of text to complete sentences. 
Aggressive scaling of these models has led to a rapid increase in their capabilities,\autocite{brown2020language,zhong2024benchmarking} with the leading models now being able to pass the BAR exam or identify and autonomously buy chemicals when augmented with external tools such as web search and synthesis planners.\autocite{openai2024gpt4}
While some see in them \enquote{sparks of \gls{agi}},\autocite{bubeck2023sparks} others consider them as \enquote{stochasitc parrots}---i.e., systems that only regurgitate what they have been trained on.\autocite{bender2021dangers}
In any case, the promise of so-called foundation models (the leading ones being called frontier models) is that they show the ability solve a wide variety of tasks they have not been explicitly trained on.\autocite{bommasani2021opportunities, anderljung2023frontier}
This led to tremendous economic interest and investment in such generative models, with an expected market of more than \$1.3 trillion (almost \$30 billion for drug discovery applications) by 2032.\autocite{bloomberg}

Chemists and materials scientists have quickly caught on the mounting attention given to \glspl{llm}, with some voices even suggesting that \enquote{the future of chemistry is language}.\autocite{White_2023}
This statement is motivated by a growing number of reports that use \glspl{llm} to predict properties of molecules or materials,\autocite{jablonka202314, jablonka2024leveraging, xie2024fine, liao2024words, zhang2024chemllm, zhong2024benchmarking} to optimize reactions\autocite{ramos2023bayesian, kristiadi2024sober}  generate materials,\autocite{rubungo2023llm, flam2023language, gruver2024fine} extract information,\autocite{Patiny_2023, Dagdelen_2024, Zheng_2024, l√°la2023paperqa, caufield2023structured} or to even build first prototypes of autonomous systems that can autonomously perform reactions in the physical world based on commands provided in natural language.\autocite{bran2023chemcrow, Boiko_2023, darvish2024organa}

In addition, since a lot---if not most---of the information about chemistry is currently stored and communicated in text there is a strong reason to believe that there is a great untapped potential in \glspl{llm} for chemistry and materials science.
For instance, most insights in chemical research does not directly originate from data stored in databases but rather from the scientists and their ability to interpret data. 
Most of these insights are in form of text in scientific publications. 
Thus, operating on such text might be our best way of unlocking these insights and to learn from them.
This might ultimately lead to general copilot systems for chemists that can provide answers to questions or even suggest new experiments based on vastly more information than a human could ever read.
Such a usage mode is especially interesting in the face of recent advanced in autonomous laboratories.\autocite{Boiko_2023, bran2023chemcrow, darvish2024organa, granda2018controlling, Angello_2022, coley2019robotic, Burger_2020, seifrid2022autonomous}


However, the rapid increase in capabilities of chemical \gls{ml} models led (even before the recent interest in \glspl{llm}) to concerns about the potential for dual use of these technologies, e.g., for the design of chemical weapons.\autocite{gopal2023releasing, ganguli2022red, Urbina_2022, campbell2023censoring, moulange2023towards, urbina2022teachable}
To some extent, this is not surprising as any technology that can be used to predict the toxicity of a compound for drug discovery, can be used in a divergent manner in chemical warfare.
Moreover, it is important to realize that the user base of such models is not only limited to experts in chemistry and materials science who critically reflect every output such models produce. 
For example, many students frequently consolidate those tools---perhaps even to prepare chemical experiments.\autocite{Intelligent.com_2023}
This also implies to users from the general public, who might consider using \glspl{llm} to answer questions about the safety of chemicals.
Thus, for some users, misleading information---especially about safety-related aspects---might lead to harmful outcomes. 
However, even for experts, the chemical understanding and reasoning capabilities are important as they will determine the capabilities and limitations of their models in their work, e.g., in copilot systems for chemists.
Unfortunately, apart from anecdotal reports, there is little evidence on how \glspl{llm} perform compared to experts.

Thus, to better understand what \glspl{llm} can do for chemistry and materials science, and where they might be improved with further developments, evaluation frameworks are needed to allow us to systematically measure progress and mitigate potential harms.
For the development of \glspl{llm}, evaluation is currently mostly performed via standardized benchmarks suites such as BigBench\autocite{srivastava2022beyond} or the LM Eval Harness.\autocite{eval-harness}
The former contains, among 204 tasks (such as linguistic puzzles), only two tasks classified as \enquote{chemistry related} whereas the latter contains no specific chemistry tasks.
Due to the lack of widely accepted standard benchmarks, the developers of chemical language models\autocite{jablonka2024leveraging, guo2023large, ahmad2022chemberta2, Cai_2024, frey2023neural} frequently utilize language-interfaced\autocite{dinh2022lift} tabular datasets such as the ones reported in MoleculeNet,\autocite{wu2018moleculenet} Therapeutic Data Commons\autocite{huang2021therapeutics} or MatBench.\autocite{dunn2020benchmarking}
In these cases, the models are evaluated on predicting very specific properties of molecules (e.g., solubility, toxicity, or reactivity) or on predicting the outcome of specific chemical reactions.
This, however, only gives a very limited view of the general chemical capabilities of the models.

While some benchmark based on university entrance exams\autocite{Zaki_2024, arora2023llms} or automatic text mining\autocite{song2023honeybee, wei2021chemistryqa} have been proposed, none of them have been widely accepted.
Likely, this is because they cannot be used automatically with black box (or tool-augmented) systems, do not cover a wide range of topics, or are not carefully validated by experts.
On top of that, the existing benchmarks are not designed to be used with models that support special treatment of molecules or equations and do not provide insights in how the models compare relative to experts.

In this work, we report a novel benchmarking framework, which we call \chembench, and use it to reveal limitations of current frontier models for the use in the chemical sciences.
Our benchmark consists of \variable{output/total_number_of_questions.txt}\xspace question-answer pairs: manually (\variable{output/manually_generated.txt}\unskip) or semi-automatically (\variable{output/automatically_generated.txt}\unskip) compiled from diverse sources.
Our corpus covers a large fraction of the topics taught in undergraduate and graduate chemistry curricula and can be used to evaluate any system that can return text (i.e., also tool-augmented systems).

To contextualize the scores, we also surveyed more than \variable{output/number_experts.txt} experts in chemistry on a subset of the benchmark corpus to be able to compare the performance of current frontier models with the one of humans.
Our results indicate that current frontier models perform \enquote{superhuman} on some aspects of chemistry but in many cases, included safety-related ones, might be very misleading.
We find that there are plenty limitations that current models still need to overcome to find use in autonomous systems for chemists and that carefully created broad benchmarks are an important stepping stone for progress in this field.  %and underline the importance of careful evaluation of the capabilities of \glspl{llm} in the chemical sciences.

\section{Results}

\subsection{Benchmark corpus}

To compile our benchmark corpus we utilized a broad list of sources (see \Cref{sec:curation}), ranging from university exams to semi-automatically generated questions based on curated subsets of data in chemical databases.
For quality assurance, all questions have been reviewed by at least one scientist in addition to the original curator as well as automatic checks. 

Importantly, our large pool of questions encompasses a wide range of topics.
This can be seen, for example, in \Cref{fig:topic_barplot} in which we compare the number of questions in different subfields of the chemical sciences (see \Cref{sec:meth-topic} for details on how we assigned topics).
The distribution of topics is also evident from \Cref{fig:question_diversity} in which we visualize the questions in a two-dimensional space using a \gls{pca} on the embeddings of the questions.
In this representation, semantically similar questions are close to each other, and we color the points based on a classification into \variable{output/num_topics.txt} topics. 
It is clear that a focus of \chembench (by design) lies on safety-related aspects which in \Cref{fig:question_diversity} appear as a large distinct cluster.


\begin{figure}[!htb]
    \centering
    \includegraphics{figures/question_count_barplot.pdf}
    \caption{\textbf{Number of questions for different topics.} The topics have been assigned using a combination of a rule-based system (mostly based on the source the question has been sampled from) as well as a classifier operating on word-embeddings of the questions. 
    The figure shows that not all aspects of chemistry are equally represented in our corpus. The \chembench corpus, by design, currently focuses on safety-related aspects, which is also evident in \Cref{fig:question_diversity}. This figure represents the combined count of \glstext{mcq} and open-ended questions.}
    \label{fig:topic_barplot}
    \script{plot_statistics.py}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/question_diversity.pdf}
    \caption{\textbf{Principal component projection of embeddings of questions in the \chembench corpus.} To obtain this figure, we embedded questions and answers using the BART model\autocite{bart} (using other embeddings, such as of OpenAI's ada model leads to qualitatively similar results). We then project the embeddings into a two-dimensional space using \gls{pca}. We color the points based on a classification into topics. Safety related aspects cover a large part of the figure that is not covered by questions from other topics.}
    \label{fig:question_diversity}
    \script{plot_question_diversity.py}
\end{figure}

While many of the existing benchmarks are designed around \gls{mcq}, this does not reflect the reality of chemistry education and research.
For this reason, \chembench samples both \gls{mcq} and open-ended questions (\variable{output/mcq_questions.txt} \gls{mcq} questions vs. \variable{output/non_mcq_questions.txt} open-ended questions).


\paragraph{\enquote{Tiny} subset}
It is important to note that for routine evaluations, a smaller subset of the corpus might be more practical.\autocite{polo2024tinybenchmarks}
For instance,~\textcite{liang2023holistic} report costs of more than 10,000 USD for \gls{api} calls for a single evaluation on the widely used \gls{helm} benchmark. 
To address this, we also provide a subset (\variable{output/num_tiny_questions.txt} questions) of the corpus that was curated to be a diverse and representative subset of the full corpus in which topics are balanced than in the complete corpus (see \Cref{sec:subset-selection} for details on the curation process).
We also used this subset to seed the web application we used for the human baseline study. 


\subsection{Model evaluation}

\paragraph{Benchmark suite design} Because the text used in scientific settings is different from typical natural language, many models have been developed that deal with such text in a special way.
For instance, Galactica\autocite{taylor2022galactica} uses special tokenization or encoding procedures for molecules and equations. 
Current benchmarking suites, however, do not account for such special treatment of scientific information.
To address this, \chembench is designed to encode the semantic meaning of various parts of the question or answer.  
For instance, molecules represented in \gls{smiles} are encapsulated in \texttt{[START\_SMILES][\textbackslash END\_SMILES]} tags. 
This allows the model to treat the \gls{smiles} strings different from other text. 
\chembench can seamlessly handle such special treatment in an easily extensible way because the questions are stored in an annotated format.

Since many widely utilized systems do only provide access to text completions (and not the raw model outputs), \chembench is designed to operate on text completions.
This is also important with the growing number of tool-augmented systems that are deemed essential for building chemical copilot systems. 
Such systems can augment the capabilities of \glspl{llm} through the use external tools such as search \glspl{api} or code executors.\autocite{schick2024toolformer, karpas2022mrkl, yao2022react}
In those cases, the \gls{llm} that returns the probabilities for various tokens (that are often used for model evaluations\autocite{Fourrier_Habib_Launay_Wolf}) is only a part of the whole system, and it is not clear how to interpret the probabilities in the context of the whole system.
The text completions, however, are the final outputs of the system that would also be used in real-world applications.

\begin{figure}[!h]
    \centering
    \includegraphics{figures/human_subset_performance.pdf}
    \caption{\textbf{Performance of models and humans on the \enquote{tiny} subset of the  \chembench corpus.} The figure shows the percentage of questions that the models answered completely correctly. We use horizontal bars to indicate the performance of various models and highlight statistics of the human performance. 
    Since the humans did not answer all the questions, this plot is based on the subset of questions that most humans answered.
    The evaluation we use here is very strict as it only considers a question answered completely correctly or completely incorrectly, partially correct answers are also considered as incorrect.
    \Cref{fig:barplot_all_correct_all_questions} gives an overview of the performance of various models on the entire corpus.
    Systems with \enquote{ReAct} in the name are tool augmented, i.e., they can call external tools such as web search or Python code executors to better answer the questions.
    However, we limit those systems to a maximum of ten calls to the \gls{llm}. This constraint led the systems to often not find the correct answer within the specified number of calls.
    In this case, we consider the answer as incorrect. 
    }
    \label{fig:human_vs_models_bar}
    \script{plot_overview_performance_plot.py}
\end{figure}

\paragraph{System performance} 
To understand the current capabilities of \glspl{llm} in the chemical sciences, we evaluated a wide range of leading models\autocite{Huggingface} on the \chembench corpus, including systems augmented with external tools.
An overview of the results of this evaluation is shown in \Cref{fig:human_vs_models_bar}. 
In this figure, we show the percentage of questions that the models answered completely correctly.
Moreover, we show the highest, average, and lowest performances of the human experts, which we obtained via a custom web application (\url{chembench.org}) that we used to survey the experts.
Remarkably, the figure shows that the leading \gls{llm}, Claude 3, outperforms the best human in our study in this overall metric and vastly, by more than a factor of two, exceeds the average performance of the experts in our study.
Many other models also outperform the average human performance. However, it is interesting to point out that the Galactica model that was trained specifically for scientific applications is outperformed by more general models.

Given the large interest in tool-augmented systems, the bad performance of these systems (GPT-3.5 and Claude 2 augmented with tools) in our benchmark is striking. 
Their lack of performance, however, is partially due to the fact that we limited the system to a maximum of 10 \gls{llm} calls.
With the default tool augmented setup (using the so-called ReAct method\autocite{yao2023react}), however, this often did not allow the system to find the correct answer (e.g., because it repeatedly tried to search for the answer on the web and did not find a solution within ten calls to the \gls{llm}).
This observation highlights the importance of communicating and measuring not only the predictive performance but also the computational cost (e.g., in terms of \gls{api} calls) for tool-augmented systems.

% Radar chart
\paragraph{Performance per topic} To obtain a more detailed understanding of the performance of the models, we also analyzed the performance of the models in different subfields of the chemical sciences.
For this analysis, we defined a set of topics (see \Cref{sec:meth-topic}) and classified all questions in the \chembench corpus into these topics based on hand-crafted rules as well as classifier models operating on the question texts.
We then computed the percentage of questions that the models, or humans, answered completely correctly for each topic.
\begin{figure}[!h]
    \centering
    \includegraphics{figures/all_questions_models_completely_correct_radar_human.pdf}
    \caption{\textbf{Performance of the models and humans on the different topics of the \enquote{tiny} subset of the \chembench corpus.} The radar plot shows the performance of the models and humans on the different topics of the \enquote{tiny} subset of the \chembench corpus. The performance is measured as the fraction of questions that were answered completely correct by the models.
    The best score for every dimension is one (all questions answered correctly) and the worst score is zero (no question answered completely correctly). A larger colored area indicates a better performance.
    This figure shows the performance on the subset of questions that was answered by humans. The performance of models on the entire corpus is shown in \Cref{fig:performance_per_topic}.
    }
    \label{fig:all_questions_models_completely_correct_radar_human}
    \script{analyze_model_reports.py}
\end{figure}
In this spider chart, the worst score for every dimension is zero (no question answered completely correctly) and the best score is one (all questions answered correctly). 
Thus, a larger colored area indicates a better performance. 
One can observe that this performance varies widely across models and topics. 
While macromolecular chemistry and biochemistry receive relatively high scores for many models, this is not the case for topics such as chemical safety or analytical chemistry.
In the subfield of analytical chemistry the prediction of the number of peaks observable in a \gls{nmr} spectrum proved difficult for the models (e.g., \variable{output/subset_scores/is_number_nmr_peaks_gpt4.txt} percent correct answers for GPT-4), while this question appeared easier (\variable{output/human_subset_scores/is_number_nmr_peaks.txt} percent correct) for trained humans.
Importantly, the human experts are given a drawing of the compounds, whereas models are only shown the \gls{smiles} string of a compound and have to use this to reason about the symmetry of the compound (i.e., to identify the number of diasterotopically distinct protons, which requires \emph{reasoning} about the topology and structure of a molecule). 
These findings also shine an interesting light on the value of textbook-inspired questions. 
A subset of the questions in \chembench are based on textbooks targeted at undergraduate students. 
On those questions the models tend to perform better than on some our semi-automatically constructed tasks (see \Cref{fig:performance_per_topic}).
For instance, while the overall performance in the chemical safety topic is low, the models would pass the examination for a chemical license based on a subset of a questions we sampled from the corresponding question bank (e.g., \variable{output/subset_scores/is_gfk_gpt4.txt}\% correct answers for GPT-4, \variable{output/subset_scores/is_gfk_claude3.txt}\% for Claude 3, and \variable{output/human_subset_scores/is_gfk.txt}\% for the human experts).
While those findings are impacted by the subset of questions we sampled, the results still highlight that good performance on such a question set or textbook questions does not necessarily translate to good performance on other questions that require more reasoning.

% Calculation vs. no calculation
Given that some of the questions in \chembench require elaborate calculations, we also analyzed if the models perform differently on those questions.
For this, we manually annotated the questions that require calculations and performed the analysis on only those questions.  % TODO add some analysis here 

\clearpage

% Estimate of difficulty
\paragraph{Confidence estimates} One might wonder whether the models are capable of estimating if they could be able to answer a question correctly. 
If they were able to do so, incorrect answers would be less problematic as one would be able to detect when an answer is incorrect.
To investigate this, we prompted\autocite{xiong2023llms} some of the top-performing models to estimate (on an ordinal scale) their confidence to answer the question correctly.

\begin{figure}[!h]
    \centering
    \includegraphics{figures/confidence_vs_performance_overall.pdf}
    \caption{\textbf{Relationship between confidence of the model in the answer and correctness.} For this analysis, we used verbalized confidence estimates from the model. To obtain those estimates, we prompted the models to return a confidence score on an ordinal scale. We then plot the correctness of the answers [which can be either completely correct (1) or completely incorrect (0)] against the confidence score.
        The stripplots show the individual data points, a strong density indicating a higher number of points. The lines show the mean and the errorbars show the standard error of the mean. The figure shows that the reliability of the confidence estimates varies widely across models. While the ones for GPT-4 and Claude-2 are not particularly reliable, the ones for Claude-3 follow the expected trend.}
    \label{fig:confidence_vs_performance}
    \script{joint_analysis_confidence_performance.py}
\end{figure}

In \Cref{fig:confidence_vs_performance} we show that for some models there is no significant correlation between the estimated difficulty and whether the models answered the question correctly or not.
For applications in which humans might rely on the models to provide answers, this is a concerning observation which highlights the need for critical reasoning in the interpretation of the models' outputs.\autocite{Li_2023}
For example, for the questions about the safety profile of compounds, GPT-4 reported an average confidence of \variable{output/model_confidence_performance/gpt4_is_pictograms_average_confidence_correct_overall.txt} (on a scale of 1--5) for the \variable{output/model_confidence_performance/gpt4_is_pictograms_num_correct_overall.txt} questions it answered in correctly and \variable{output/model_confidence_performance/gpt4_is_pictograms_average_confidence_incorrect_overall.txt} for the \variable{output/model_confidence_performance/gpt4_is_pictograms_num_incorrect_overall.txt} questions it answered incorrectly.
While, on average, the verbalized confidence estimates from Claude 3 seem better calibrated (\Cref{fig:confidence_vs_performance}), they are still misleading for the questions about \gls{ghs} pictograms with an average score of \variable{output/model_confidence_performance/claude3_is_pictograms_average_confidence_correct_overall.txt} for correct answers and \variable{output/model_confidence_performance/claude3_is_pictograms_average_confidence_incorrect_overall.txt} for incorrect answers.

\section{Discussion and Conclusions}
Our findings show why \glspl{llm} are catching the interest of scientists across domains, but also the attention of the general public.
As observed, leading models outperform domain experts in chemistry questions on a majority of topics.
Unsurprinsigly, there are still striking limitations. 
First, it is important to note that the models are not able to estimate their own limitations.
Moreover, on very relevant topics (such as GHS classification) the answers provided by the models are mostly inaccurate.
Looking in more detail at the relationship between the model performance and the question type/source revealed that the studied models perform well on textbook questions, while underperforming on questions that require more reasoning (e.g. questions that require calculations).

Yet, given that the models outperform the average human in our study, it is clear that we need to rethink the way we teach and examine chemistry.
Critical reasoning is increasingly important and robotic solving of problems or memorization of facts is clearly a domain in which \glspl{llm} will continue to outperform humans.

Our findings also highlight the nuanced tradeoff between breadth and depth of evaluation frameworks. 
The analysis of model performance on different topics shows that the performance of models varies widely across topics. 
But even within a topic, the performance of models can vary significantly depending on the type of question and the reasoning required to answer it.

The current evaluation frameworks for \glspl{llm} are mostly designed to measure the performance of the models on specific tasks. 
They cannot easily be used to evaluate systems, or models built for scientific applications. As a result, we only had limited understanding of the capabilities of \glspl{llm} in the chemical sciences.
Our work shows that carefully curated benchmarks can provide a deeper understanding of the capabilities of \glspl{llm} in the chemical sciences.
It lays the groundwork for the development of more advanced systems that can be used in the chemical sciences and highlights the importance of continued development of evaluation frameworks for systematic improvement of safe and useful \glspl{llm}.
Our findings also illustrate that more focus is needed in the development of better human-model interaction frameworks given that models are not able to estimate their own limitations.

While our findings indicate many areas for further improvement of \gls{llm}-based systems, it is also important to realize that clearly defined metrics have been the key for the progress of many fields of \gls{ml}, such as computer vision. 
We believe that while current systems are far from reasoning like a chemist, our open-source \chembench framework will be a stepping stone for the development of systems that might come closer to this goal.

\clearpage

\section{Methods}

\subsection{Curation workflow}\label{sec:curation}
For our dataset we curated questions from existing exams, but also programmatically created new questions.
Questions were added via Pull Requests on our GitHub repository and only merged into the corpus after passing manual review.

To ensure that the questions do not enter a training dataset we use the same canary string as the BIG-bench project.
This requires that \Gls{llm} developers filter their training dataset for this canary string.\autocite{openai2024gpt4, srivastava2022beyond}

\begin{figure}[!h]
    \includegraphics[width = \textwidth]{figures/chem-bench.pdf}
    \caption{\textbf{Overview of the workflow for the assembly of the \chembench corpus}. To assemble the \chembench corpus, we first collected questions from various sources. Some tasks were manually curated, others semi-programmatically. For all questions we added semantic annotations to make them compatible with systems that use special processing for modalities that are not conventional natural text. Before adding the questions to the corpus, we reviewed with manual as well as automatic methods.}
    \label{fig:curation_workflow}
\end{figure}

\paragraph{Manually curated questions}

\subparagraph{Analytical chemistry}
\variable{output/question_count_per_dir/json_file_counts_analytical_chemistry.txt} questions are based on master student examination papers at the University of Jena (Germany). 
These questions focus on the principles and techniques employed in analytical chemistry.

\subparagraph{Combustion engineering}
\variable{output/question_count_per_dir/json_file_counts_combustion_engineering.txt} question are based on master student examination papers at the University of Magdeburg (Germany). 
They explore topics related to combustion processes and engineering principles.

\subparagraph{Functional materials and nanomaterials}
\variable{output/question_count_per_dir/json_file_counts_func_mats_and_nanomats.txt}  questions are based on exercises from a seminar conducted at the University of Jena (Germany), delving into topics concerning functional materials and nanomaterials, exploring their properties and applications.

\subparagraph{General chemistry}
\variable{output/question_count_per_dir/json_file_counts_Gen_Chem_MCA.txt} questions originate from examinations for grade 10 at Marist Comprehensive Academy Uturu (Nigeria). 
They cover fundamental concepts in chemistry suitable for students at the secondary school level.
In addition, we included questions based on the general chemistry courses of the Bachelor of Science in Chemistry at the Technical University of Munich (Germany), focusing on inorganic chemistry (\variable{output/question_count_per_dir/json_file_counts_ac_faessler_tum.txt}) and principles of chemistry (\variable{output/question_count_per_dir/json_file_counts_pum_tum.txt}).

\subparagraph{Chemistry Olympiads}
\variable{output/question_count_per_dir/json_file_counts_icho.txt} questions are based on various local and international chemistry competitions, including those held in the USA, UK, and Moldova.

\subparagraph{Material synthesis}
\variable{output/question_count_per_dir/json_file_counts_materials_synthesis.txt} questions are based on seminars on material synthesis at the University of Jena (Germany), focusing on the methods and techniques employed in the synthesis of various materials.

\subparagraph{Organic reactivity}
\variable{output/question_count_per_dir/json_file_counts_organic_reactivity.txt}  questions are primarily sourced from tutorials and exam papers for the Bachelor of Science in Chemistry at the Technical University of Munich (Germany), exploring reactions and mechanisms in organic chemistry.

\subparagraph{Periodic table}
\variable{output/question_count_per_dir/json_file_counts_periodic_table_properties.txt} questions are manually created to cover various aspects and trends of the periodic table.

\subparagraph{Polymer chemistry}
\variable{output/question_count_per_dir/json_file_counts_polymer_chemistry.txt} questions are based on examinations held at the University of Hamburg (Germany), covering topics related to polymer chemistry and its applications.

\subparagraph{Textbook questions}
\variable{output/question_count_per_dir/json_file_counts_oup.txt} questions are based on questions from various textbooks covering biomolecular science, drug synthesis, molecular structure, organic chemistry, and X-ray crystallography.

\subparagraph{Chemical safety} 
This category comprises a total of \variable{output/question_count_per_dir/json_file_counts_safety.txt} questions, including semi-programmatically created questions covering areas such as GHS classification, hazard statements, \gls{dai}, and pictograms from the PubChem database \autocite{pubchem} (see below for details). 
Additionally, it explores crucial topics like materials' compatibility (\variable{output/question_count_per_dir/json_file_counts_materials_compatibility.txt} questions) and chemical compatibility (\variable{output/question_count_per_dir/json_file_counts_chem_chem_comp.txt} questions), essential for understanding the hazardous impact of mixing certain materials and chemicals. 
Furthermore, it includes \variable{output/question_count_per_dir/json_file_counts_blac_gfk.txt} questions based on the question bank published by the Federal/State Working Group on Chemical Safety (BLAC), which is used for the expertise examination (\enquote{Sachkundepr√ºfung}) according to ¬ß11 of the Chemical Prohibition Ordinance (\enquote{Chemikalien-Verbotsverordnung}).
Moreover, this category includes questions about lab safety, pharmacology, toxicology, material safety data sheets, and chemical safety data sheets. 
The chemical safety category stands out as one of the primary focus areas within the \chembench corpus.

\subparagraph{NMR spectroscopy}
Questions are based on images shared via the Twitter account \url{https://twitter.com/NMRspectroscopy}, focusing on the choice and design of appropriate \gls{nmr} experiments.


\paragraph{Semi-programatically generated questions}

\subparagraph{Chemical reactivity}
\variable{output/question_count_per_dir/json_file_counts_reactive_groups.txt} questions are framed based on information from the Cameo Chemicals website (\url{https://cameochemicals.noaa.gov/reactivity}), exploring the reactivity of various chemicals and compounds.


\subparagraph{Oxidation states}
\variable{output/question_count_per_dir/json_file_counts_oxidation_states.txt} questions regarding oxidation states are based on data from the website \url{https://www.cheminfo.org/}. 

\subparagraph{Total electron count of molecules}
\variable{output/question_count_per_dir/json_file_counts_electron_counts.txt} questions pertaining to the total electron count of molecules are based on data from the website \url{https://www.cheminfo.org/}. 

\subparagraph{Number of isomers}
We used MAYGEN\autocite{Yirik_2021} to compute the number of isomers for a set of \gls{smiles} randomly sampled from the ZINC dataset.\autocite{Irwin_2012}
To obtain the answers for false options we sampled from the set of all observed isomer counts. In total, we generated \variable{output/question_count_per_dir/json_file_counts_number_of_isomers.txt} questions.

\subparagraph{Point group for molecules}
\variable{output/question_count_per_dir/json_file_counts_point_group.txt} questions involve predicting the point group for molecules. 
Our ChemCaption tool (\url{https://github.com/lamalab-org/chem-caption}) is utilized for data generation. 
Given a molecule, it uses spglib\autocite{spglib} to identify the point group. 
Assigned point groups were manually verified to only include well-defined cases.

\subparagraph{SMILES-IUPAC}
We scraped pairs of \gls{smiles} and \gls{iupac} names from the PubChem database \autocite{pubchem} and then framed \variable{output/question_count_per_dir/json_file_counts_smiles_to_name.txt} questions around the task of converting \gls{smiles} notation to \gls{iupac} nomenclature.
\variable{output/question_count_per_dir/json_file_counts_name_to_smiles.txt} questions involve the reverse task, i.e., converting \gls{iupac} nomenclature to \gls{smiles} notation.


\subparagraph{Number of NMR peaks} 
To generate the \variable{output/question_count_per_dir/json_file_counts_name_to_smiles.txt}  tasks about the number of \gls{nmr} peaks, we randomly sampled \gls{smiles} from the ZINC database\autocite{Irwin_2012} and then used OpenChemLib\autocite{openchemlib} to compute the number of diasterotopically distinct hydrogen atoms. 
We then sampled from the set of number of peaks (excluding the correct answer) to obtain the false answer options.

\subparagraph{GHS, hazard statements and DAI}
The \gls{ghs} classification, hazard statements and \gls{dai} data have been extracted from PubChem.\autocite{pubchem}
The former two have been mined via the PubChem \gls{api}, while the latter has been manually compiled. 
In total, we generated \variable{output/question_count_per_dir/json_file_counts_pictograms.txt} questions about the \gls{ghs} classification of chemicals, \variable{output/question_count_per_dir/json_file_counts_h_statements.txt} questions about definition of hazard statements, and \variable{output/question_count_per_dir/json_file_counts_dai.txt} questions about \glspl{dai}.
The \glspl{dai} have been curated to contain only records approved by the \gls{who}.
The chemicals in this class of questions belong to one of the three classes: pesticides (e.g., calcium arsenate), insecticides (e.g., cyfluthrin) or herbicides (e.g. 2,4-D).


\subsection{Model evaluation workflow}

\paragraph{Prompting}

To maintain consistency with the training, we employ distinct prompt templates tailored for completion and instruction-tuned models. 
We impose constraints on the models within these templates to receive response in a specific format so that a robust, fair and consistent parsing can be performed as explained below.
Certain models are trained with special annotations and \LaTeX\xspace syntax for scientific notations, chemical reactions, or symbols embedded within the text. 
For example, all the SMILES representations are encapsulated within \texttt{[START\_SMILES][\textbackslash END\_SMILES]} in Galactica\autocite{taylor2022galactica}.
Our prompting strategy consistently adheres to  these details in a model-specific manner by post-processing \LaTeX\xspace syntax, chemical symbols, chemical equations and physical units (by either adding or removing wrappers).
This step can be easily customized in our codebase.



\paragraph{Parsing}
Our parsing workflow is multistep and primarily based on regular expressions.
In the case of instruction-tuned models we first attempt to identify the \texttt{[ANSWER][\textbackslash ANSWER]} environment we prompt the model to report the answer in.
In the case of completion models, this step is skipped. From there, we attempt to extract the relevant enumeration letters (for multiple-choice questions) or numbers.
In the case of numbers, our regular expression was engineered to be able to deal with various forms of scientific notation.
As initial tests indicated that models sometimes return integers in the form of words, e.g. \enquote{one} instead of \enquote{1}, we also implemented a word-to-number conversion using regular expressions.
In case these hard-coded parsing steps fail, we fall back to using a \gls{llm}, e.g. Claude-2, to parse the completion.
%Manual verification indicates that this \gls{llm}-based parsing is not a relevant error source and has been performed correctly in all cases we checked (randomly sampled 4 questions per topic).

\input{sections/parse_check_desc.tex}

\paragraph{Models}
We find that the performance of models depends on the sampling temperature. Typically we find the best temperature with greedy argmax decoding ($T=0$).
\subparagraph{Completion models}
We used Galactica (120b)\autocite{taylor2022galactica} with the default settings.


\subparagraph{Instruction-tuned models} In addition, we used Claude 2, Claude3 (Opus),\autocite{anthropicClaudeModelFamily2024} GPT-4,\autocite{openai2024gpt4} GPT-3.5-turbo,\autocite{brown2020language} Gemini Pro,\autocite{gemini} Mixtral-8x7b\autocite{jiang2024mixtral} Llama2 (70b),\autocite{touvron2023llama} as well as the 7B chat model from Perplexity.AI.

\subparagraph{Tool augmented models}
In addition to directly prompting \glspl{llm}, we also investigated the performance of tool-augmented systems.
For this we investigated the 7B parameter \enquote{online} model of Perplexity.AI. 
In addition, we used gpt-3.5-turbo and Claude 2 with ReAct-style tool augmentation.\autocite{yao2023react}
The latter two models had access to WolframAlpha, the ArXiv \gls{api}, a Python interpreter, as well as web search (through DuckDuckGo).
We implemented the system using Langchain with the default prompts and constrained the system to a maximum of ten \gls{llm} calls.


\subsection{Confidence estimate}
To estimate the models' confidence we prompted them with the question (and answer options for \gls{mcq}) and the task to rate their confidence to produce the right answer on a scale from 1 to 5. 
We decided to use verbalized confidence estimates\autocite{xiong2023llms} since we found those closer to current practical use cases compared to other prompting strategies, which might be more suitable when implemented in systems.

\subsection{Human baseline}

\paragraph{Question selection} \label{sec:subset-selection}
Since we anticipated that we will not be able to collect enough responses for every question to allow for a meaningful statistical analysis, we decided on showing a relevant subset of all questions to the human scorers.
For selecting the subset, we decided on addressing two questions:
\begin{itemize}
    \item Are the questions for which the models scored poorly just too difficult or unanswerable?
    \item Are there areas in which the performance of humans is very different from the ones of the models?
\end{itemize}
To answer the first question we selected up to 13 questions per source that all \glspl{llm} (model names) from an initial scoring round did not answer correctly.
In addition, we picked 100 diverse ones using the greedy MaxMin sampling on the embeddings of the questions computed using BART and added five random questions about the number of \gls{nmr} peaks and the point group of molecules.
% https://github.com/lamalab-org/chem-bench-app/issues/92 has the details 

\paragraph{Study design}
For our initial study we wanted to maximize the response rate given our available resources. 
For this reason, we did not opt for a highly controlled study setting. 
That is, while users were prompted to not use external tools other than a calculator and to not consult other humans, we do not have any way to verify that the participants complied with those rules (note that users were also allowed to skip questions). Another aspect of requiring unsupervised question answering is the real world equivalent of our benchmark, where humans have unencumbered access to tools and are able to answer any question of interest.

\paragraph{Participants}
Users were open to report about their experience in chemistry. 
Overall, \variable{output/num_users_with_education_info.txt} did so. 
Out of those, \variable{output/num_human_phd.txt} reported to have been awarded a doctoral degree.
\variable{output/num_human_postdoc.txt} are beyond a first postdoctorate, \variable{output/num_human_master.txt} have a master's degree, and \variable{output/num_human_bachelor.txt} have a bachelor's degree.


\paragraph{Comparison with models}
To compare the performance of humans (who might have answered only some questions) with the performance of models (which answered all questions), we focussed on questions which at least four humans answered and limited the pool of human scorers to those who answered at least 100 questions (i.e., \variable{output/num_humans_with_more_than_100_scores.txt} humans). 
The latter threshold was chosen to limit it to humans who made serious attempts at systematically answering a part of the questions. 
This analysis might lead to potential biases, most likely in favor of humans as they were allowed to skip questions. \variable{output/num_humans_with_204_scores.txt} humans answered more than 200 questions.
For the analysis, we treated each human as a model. For analyses grouped by topic, we computed the topic aggregated averages per human and then averaged over all humans.

\subsection{Classification of questions into topics}\label{sec:meth-topic} When curating our dataset we systematically recorded keywords and sources.
To allow for analysis of the model performance as a function of the topic, we leverage this information together with the output of sequence classification models.
For questions which can easily be assigned to a topic based on the source (e.g., number of \gls{nmr} peaks, chemical compatibility, toxicology exam questions) we use this information to make the assignment.
For the remaining ones, e.g., from chemistry olympiad questions, we use zero-short sequence classification\autocite{zeroshotsequence} using the BART model\autocite{bart, FacebookBART}, which our preliminary analysis found to be more robust than topic modeling based on embeddings from OpenAI's \texttt{ada} model or Cohere's \texttt{Cohembed-english-v3.0} model.


\section*{Data and code availability}
The code and data for \chembench is available at \url{https://github.com/lamalab-org/chem-bench} and archived at \url{XXX}.
The code for the app for our human baseline study is available at \url{https://github.com/lamalab-org/chem-bench-app}. 
To ensure reproducibility, this manuscript was generated using the \href{https://show-your.work/en/latest/}{\\showyourwork} framework.\autocite{Luger2021}
The code to rebuild the paper (including code for all figures and numbers next to which there is a GitHub icon) can be found at \url{\GitHubURL}. 
To facilitate reproduction, some intermediate results of the analysis are cached at \url{http://dx.doi.org/10.5072/zenodo.34706}.

\section*{Acknowledgements}
This work was supported by the Carl Zeiss Foundation and a \enquote{Talent Fund} of the \enquote{Life} profile line of the Friedrich Schiller University Jena.
We thank Stability.AI for the use of its HPC cluster and for its generous donations.
We also thank the OpenBioML.org community and their ChemNLP project team for useful discussions.

M.A. expresses gratitude to the European Research Council (ERC) for evaluating the project with the reference number 101106377 titled \enquote{CLARIFIER} and accepting it for funding under the HORIZON TMA MSCA Postdoctoral Fellowships - European Fellowships. 
Furthermore, M.A. acknowledges the funding provided by UK Research and Innovation (UKRI) under the UK government‚Äôs Horizon Europe funding guarantee (Grant Reference: EP/Y023447/1; Organization Reference: 101106377).

The authors thank Julian Kimmig for feedback on the web app. 

\section*{Conflicts of interest}
K.M.J.\ is a paid consultant for OpenAI (as part of the red teaming network). M.P.\ is employee of Stability.AI and A.M.\ and N.A.\ are paid contractors of Stability.AI.

\section*{Author contributions}

\scriptsize
\insertcredits
\normalsize
\printbibliography
\end{refsection}


\clearpage
\begin{refsection}
\appendix
\section{Appendix}
\input{appendix.tex}
\clearpage
\printbibliography[heading=subbibintoc]
\end{refsection}
\end{document}
