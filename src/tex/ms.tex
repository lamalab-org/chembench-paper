\documentclass[11pt, oneside]{article}

\input{preamble.tex}
\usepackage{credits}
\usepackage{orcidlink}
\usepackage{showyourwork}


\title{\textsf{Are frontier models superhuman chemists?}}

\input{authors.tex}
\begin{document}
\maketitle

\begin{abstract}
    Large language models (LLMs) have found widespread interest as they can perform tasks they have not been explicitly been trained on. 
    This is relevant for the chemical sciences, which face the problem of learning from many small and diverse datasets that are frequently in the form of text.
    LLMs are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously.

    However, we still have only very limited systematic understanding of the chemical reasoning capabilities of LLMs, which would be required to improve models and mitigate potential harms. 
    Here, we introduce \enquote{\chembench,} an automated framework designed to rigorously evaluate the chemical knowledge and reasoning abilities of state-of-the-art LLMs, including GPT-4, against the expertise of human chemists.

    We curated more than 6000 question answer pairs about a wide array of subfields, evaluated leading models such as GPT-4, and found that the best models, on average, outperform the best human chemists in our study. 
    The models, however, struggle with some chemical reasoning tasks that are easy for human experts and give overconfident misleading predictions, e.g., about the safety profile of chemicals. 

    These findings underscore the dual reality that, although LLMs demonstrate remarkable proficiency in chemical tasks, there is a critical need for further research to enhance their safety and utility in chemical sciences.
    Our findings also indicate a need for adaptation to chemistry curricula and highlight the importance of continued development of evaluation frameworks for systematic improvement of safe and useful LLMs.
\end{abstract}

\clearpage

\section{Introduction}
\Glspl{llm} are \gls{ml} models trained on massive amounts of text to complete sentences. 
Aggressive scaling of the models has led to a rapid increase in their capabilities,\cite{brown2020language} with the leading models now being able to pass the BAR exam or identify and autonomously buy chemicals when augmented with external tools such as web search and synthesis planners.\cite{openai2024gpt4}
While some see in them \enquote{sparks of \gls{agi}},\cite{bubeck2023sparks} others consider them as \enquote{stochasitc parrots}---i.e., systems that only regurgitate what they have been trained on.\cite{bender2021dangers}
In any case, the promise of so-called foundation models (the leading ones being called frontier models) is that they show the ability solve a wide variety of tasks they have not been explicitly trained on.\cite{bommasani2021opportunities, anderljung2023frontier}
This led to tremendous economic interest and investment in those generative models, with an expected market of more than \$1.3 trillion (almost \$30 billion for drug discovery applications) by 2032.\cite{bloomberg}

Chemists and materials scientists have quickly caught on the mounting attention given to \glspl{llm}, with some voices even suggesting that \enquote{the future of chemistry is language}.\cite{White_2023}
This statement is motivated by a growing number of reports that use \glspl{llm} to properties of molecules or materials,\cite{jablonka202314, jablonka2024leveraging, xie2024fine, liao2024words, zhang2024chemllm, zhong2024benchmarking} to optimize reactions\cite{ramos2023bayesian, kristiadi2024sober} or generate materials,\cite{rubungo2023llm, flam2023language, gruver2024fine} extract information,\cite{Patiny_2023, Dagdelen_2024, Zheng_2024, l√°la2023paperqa, caufield2023structured} or to even build \enquote{autonomous} systems that can autonomously perform reactions in the physical world based on commands provided in natural language.\cite{bran2023chemcrow, Boiko_2023, darvish2024organa}
But even more than that, since a lot---if not most---of the information about chemistry is currently stored and communicated in text there is a strong reason to believe that there is a lot of untapped potential in \glspl{llm} for chemistry and materials science.
For instance, most insights in chemical research does not directly originate from data stored in databases but rather from the scientists and their ability to interpret data. 
Most of these insights are in form of text in scientific publications. 
Thus, operating on such text might be our best way of unlocking and \emph{learning} from these insights.
This might ultimately lead to copilot systems for chemists that can provide answers to questions or even suggest new experiments based on vastly more information than a human could ever read.
Such a usage mode is especially interesting in the face of recent advanced in autonomous laboratories.\cite{Boiko_2023, bran2023chemcrow, darvish2024organa, granda2018controlling, Angello_2022, coley2019robotic, Burger_2020, seifrid2022autonomous}
However, the rapid increase in capabilities of chemical \gls{ml} models led (even before the recent interest in \glspl{llm}) to concerns about the potential for dual use of these technologies, e.g., for the design of chemical weapons.\cite{gopal2023releasing, ganguli2022red, Urbina_2022, campbell2023censoring, moulange2023towards, urbina2022teachable}
To some extent, this is not surprising as any technology, that, for instance, can predict the toxicity of molecules (to be used in drug discovery) can also be used to predict the toxicity of molecules (to be used in chemical warfare).

It is important to realize that the user base of such models is not only limited to experts in chemistry and materials science that reflect every output such models produce. 
For example, many students frequently consolidate those tools---perhaps even to prepare chemical experiments.\cite{Intelligent.com_2023}
This also implies to users from the general public, who might consider using \glspl{llm} to answer questions about the safety of chemicals.
Thus, for some users, misleading information---especially about safety-related aspects---might lead to harmful outcomes. 
However, even for experts the chemical understanding and reasoning capabilities are important as they will determine the capabilities and limitations of their models in their work, e.g., in copilot systems for chemists.
Unfortunately, apart from anecdotal reports there is little evidence on how \glspl{llm} perform compared to experts.

Thus, to better understand what \glspl{llm} can do for chemistry and materials science, and where they might be improved with further developments, evaluation frameworks are needed to allow us to systematically measure progress and mitigate potential harms.
For the development of \glspl{llm}, evaluation is currently mostly performed via standardized benchmarks suites such as BigBench\cite{srivastava2022beyond} or the LM Eval Harness.\cite{eval-harness}
The former contains, among 204 tasks (such as linguistic puzzles), only two tasks classified as \enquote{chemistry related} whereas the latter contains no specific chemistry tasks.
Due to the lack of widely excepted standard benchmarks, the developers of chemical language models\cite{jablonka2024leveraging, guo2023large, ahmad2022chemberta2, Cai_2024, frey2023neural} frequently utilize language-interfaced\cite{dinh2022lift} tabular datasets such as the ones reported in MoleculeNet,\cite{wu2018moleculenet} Therapeutic Data Commons\cite{huang2021therapeutics} or MatBench.\cite{dunn2020benchmarking}
In these cases, the models are  evaluated on predicting very specific properties of molecules (e.g., solubility, toxicity, or reactivity) or on predicting the outcome of specific chemical reactions.
This, however, only gives a very limited view of the general chemical capabilities of the models.

While some benchmark based on university entrance exams\cite{Zaki_2024, arora2023llms} or automatic text mining\cite{song2023honeybee, wei2021chemistryqa} have been proposed, none of them have been widely accepted.
This is likely because they cannot automatically be used with black box (or tool-augmented) systems, do not cover a wide range of topics, or are not carefully validated by experts.
On top of that, the existing benchmarks are not designed to be used with models that support special treatment of molecules or equations and do not provide insights in how the models compare relative to experts.

In this work, we report a novel benchmarking framework, which we call \chembench, and use it to reveal limitations of current frontier models for the use in the chemical sciences.
Our benchmark consists of \variable{output/total_number_of_questions.txt} question answer pairs manually (\variable{output/manually_generated.txt}\unskip) or semi-automatically (\variable{output/automatically_generated.txt}\unskip) compiled from diverse sources.
Our corpus large fraction of the topics taught in undergraduate and graduate chemistry curricula and can be used to evaluate any system that can return text (i.e., also tool-augmented systems).

To contextualize the scores, we also surveyed more than \variable{output/number_experts.txt} experts in chemistry (for a total of number of more than \variable{output/total_hours.txt}\unskip) on a subset of the benchmark corpus to be able to compare the performance of current frontier models with the one of humans.
Our results indicate that current frontier models perform \enquote{superhuman} on some aspects of chemistry but in many cases, included safety-related ones, might be very misleading.
The results indicate the limitations that current models still need to overcome to find use in autonomous systems for chemists and underline the importance of careful evaluation of the capabilities of \glspl{llm} in the chemical sciences.

\section{Results}

\subsection{Benchmark corpus}

To compile our benchmark corpus we utilized a broad list of sources (see \Cref{sec:curation}), ranging from university exams to semi-automatically generated questions based on curated subsets of data in chemical databases.
For quality assurance, all questions have been reviewed by at least one scientist in addition to the original curator. 

Importantly, our large pool of questions encompasses a wide range of topics.
This can be seen, for example, in \Cref{fig:topic_barplot} in which we compare the number of questions in different subfields of the chemical sciences (see \Cref{sec:meth-topic} for details on how we assigned topics).
The distribution of topics is also evident from \Cref{fig:pca} in which we visualize the questions in a two-dimensional space using a \gls{pca} on the embeddings of the questions.
In this representation, semantically similar questions are close to each other, and we color the points based on a classification into \variable{output/num_topics.txt} topics. 
It is clear that a focus of \chembench (by design) lies on safety-related aspects which in \Cref{fig:pca} appear as large distinct clusters.


\begin{figure}
    \centering
    \includegraphics{figures/question_count_barplot.pdf}
    \caption{\textbf{Number of questions for different topics.} The topics have been assigned using a combination of a rule-based system (mostly based on the source the question has been sampled from) as well as a classifier operating on a word-embedding of the question. 
    The figure shows that not all aspects of chemistry are equally represented in our corpus. The \chembench corpus, by design, currently focuses on safety-related aspects, which is also evident in \Cref{fig:question_diversity}. This figure represents the combined count of \glstext{mcq} and open-ended questions.}
    \label{fig:topic_barplot}
    \script{plot_statistics.py}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figures/question_diversity.pdf}
    \caption{\textbf{Principal component projection of embeddings of questions in the \chembench corpus.} To obtain this figure, we embedded question and answers using the BART model (using other embeddings, such as of OpenAI's ada model leads to qualitatively similar results). We then project the embeddings into a two-dimensional space using \gls{pca}. We color the points based on a classification into topics.}
    \label{fig:question_diversity}
    \script{plot_question_diversity.py}
\end{figure}

While many of the existing benchmarks are designed around \gls{mcq}, this does not reflect the reality of chemistry education and research.
For this reason, \chembench samples both \gls{mcq} and open-ended questions (\variable{output/mcq_questions.txt} \gls{mcq} questions vs. \variable{output/non_mcq_questions.txt} open-ended questions).
%As one might expect, most questions are difficult to read according to the Flesch‚ÄìKincaid readability test (\variable{output/flesch_kincaid_reading_ease.txt}).\cite{kincaid1975derivation}


\paragraph{\enquote{Tiny} subset}
It is important to note that for routine evaluations, a smaller subset of the corpus might be more practical.\cite{polo2024tinybenchmarks}
For instance,~\citet{liang2023holistic} report costs of more than 10,000 USD for \gls{api} calls for a single evaluation on the \gls{helm} benchmark on a model. 
To address this, we also provide a subset (\variable{output/num_tiny_questions.txt} questions) of the corpus that was curated to be a diverse and representative subset of the full corpus (see \Cref{sec:subset-selection} for details on the curation process).
We also used this subset to seed the app we used for the human baseline study. 


\subsection{Model evaluation}

Because the text used in scientific settings is different from typical natural language, many models have been developed that deal with such text in a special way.
For instance, Galactica\cite{taylor2022galactica} uses special tokenization or encoding procedures for molecules or equations.
To support this, \chembench is designed to encode the semantic meaning of various parts of the question or answer.  
For instance, molecules represented in \gls{smiles} are encapsulated in \texttt{[START\_SMILES][\textbackslash END\_SMILES]}.

Since many widely utilized systems do only provide access to text completions (and not the raw model outputs), \chembench is designed to operate on text completions.
This is also important with the growing number of tool-augmented systems that use external tools such as search \glspl{api} or code executors.\cite{schick2024toolformer, karpas2022mrkl, yao2022react}
In those cases, the \gls{llm} that returns the probabilities for various tokens (that are often used for model evaluations\cite{Fourrier_Habib_Launay_Wolf}) is only a part of the whole system, and it is not clear how to interpret the probabilities in the context of the whole system.
The text completions, however, are the final outputs of the system that would also be used in a real-world application (e.g., in a copilot system for chemists).

To understand the current capabilities of \glspl{llm} in the chemical sciences, we evaluated a wide range of leading models\cite{Huggingface} on the chembench corpus, including systems augmented with external tools.
An overview of the results of this evaluation is shown in \Cref{fig:human_vs_models_bar}. 
In this figure, we show the percentage of questions that the models answered completely correctly.
We use horizontal bars to indicate the performance of various models and highlight statistics (e.g., the average) of the human performance with vertical lines.
Remarkably, the figure shows that the leading \gls{llm}, GPT-4, outperforms the best human in our study in this overall metric and vastly exceeds the average performance of our human scores.
Many other models also outperform the average human performance. It is, however, interesting to note that the Galactica model that was trained specifically for scientific applications is outperformed by more general models.
% Bar chart

\begin{figure}
    \centering
    \includegraphics{figures/human_subset_performance.pdf}
    \caption{\textbf{Performance of models and humans on chembench.} The figure shows the percentage of questions that the models answered completely correctly. We use horizontal bars to indicate the performance of various models and highlight statistics of the human performance. 
    Since the humans did not answer all the questions, this plot is based on the subset of questions that most humans answered.
    The metric we use here is harsh as it only considers a question answered completely correctly or completely incorrectly, partially correct answers are also considered as incorrect.
    \Cref{fig:barplot_all_correct_all_questions} gives an overview of the performance of various models on the entire corpus.
    }
    \label{fig:human_vs_models_bar}
    \script{plot_overview_performance_plot.py}
\end{figure}

Given the large interest in tool-augmented systems, the bad performance of these systems in our benchmark is striking. 
Their lack of performance, however, is partially due to the fact that we limited the system to a maximum of ten \gls{llm} calls.
With the default tool augmented setup (using the so-called ReAct method\cite{yao2023react}), however, this often did not allow the system to find the correct answer (e.g., because it repeatedly tried to search for the answer on the web).
This observation highlights the importance of transparency not only in predictive performance but also computational cost (e.g., in terms of \gls{api} calls) for tool-augmented systems.

%% Note also while the performance of the systems is better than what humans do, it is still far from perfect 

% Radar chart
To obtain a more detailed understanding of the performance of the models, we also analyzed the performance of the models in different subfields of the chemical sciences.
For this analysis, we defined a set of topics (see \Cref{sec:meth-topic}) and classified the questions into these topics based and hand-crafted rules as well as classifier models.
We then computed the percentage of questions that the models, or humans, answered completely correctly for each topic.



In this spider chart, the worst score for every dimension is zero (no question answered completely correctly) and the best score is one (all questions answered correctly). 
Thus, a larger colored area indicates a better performance. 
One can observe that this performance varies widely across models and topics. 
While computational chemistry and biochemistry receive relatively high scores for many models, this is not the cases for topics such as chemical safety or analytical chemistry.
In the subfield of analytical chemistry the prediction of the number of peaks observable in a \gls{nmr} spectrum proved difficult for the models (e.g., \variable{output/subset_scores/is_number_nmr_peaks_gpt4.txt} percent correct answers for GPT-4) 
While this question is relatively easy (\variable{output/human_subset_scores/is_number_nmr_peaks.txt}) for trained humans given a drawing of the compounds, models that are only shown the \gls{smiles} string of a compound have to use this to reason about the symmetry of the compound. 
This is mirrored in the questions about the number of isomers, which is models struggle with more than humans (\variable{output/subset_scores/is_number_of_isomers_gpt4.txt} percent for GPT-4 vs. \variable{output/human_subset_scores/is_number_of_isomers.txt}).
These findings also shin an interesting light on the value of textbook-inspired questions. A subset of the questions in the \chembench are based on textbooks targeted at undergraduate students. 
On those questions the models tend to perform better than on some our semi-automatically constructed tasks.

% GFK performance -> would the models obtain a chemical license 
% Interesting twist perhaps: the performance on the university textbooks or exams is better than on the ones we automatically constructed

% Calculation vs. no calculation
Given that some of the questions in \chembench require elaborate calculations, we also analyzed if the models perform differently on those questions.
For this, we manually annotated the questions that require calculations and performed the analysis on only those questions. 

\begin{figure}
    \centering
    %\includegraphics{figures/chem-bench-completley-correct-calculation-vs-no-calculation.pdf}
    \caption{Caption}
    \label{fig:calculation_performance}
\end{figure}

% Estimate of difficulty
One might wonder whether the models are capable of estimating if they will be able to answer a question correctly. 
If they were able to do so, incorrect answers would be less problematic as one would be able to detect when an answer is incorrect.
To investigate this, we prompted\cite{xiong2023llms} some of the top-performing models to estimate the on an ordinal scale their confidence in their ability to answer the question correctly.

\begin{figure}
    \centering
    %\includegraphics{figures/estimated_difficulty.pdf}
    \caption{Caption}
    \label{fig:estimated_difficulty}
\end{figure}

In \Cref{fig:estimated_difficulty} we show that there is no significant correlation between the estimated difficulty and whether the models answered the question correctly or not.
For applications in which humans might rely on the models to provide answers, this is a concerning observation highlighting the need for critical reasoning in the interpretation of the models' outputs.\cite{Li_2023}
For example, for the questions about the safety profile of compounds, GPT-4 reported an average confidence of XX (on a scale of 1--5) for the XX (out of XX) questions it answered in correctly (XX for the XX questions it answered correctly).

\section{Discussion and Conclusions}
Our findings also lay the groundwork for discussions for the chemical education in the future. 
Given that the models outperform the average human in our study, it is clear that we need to rethink the way we teach and examine chemistry.
Critical reasoning is increasingly important and rote solving of problems or memorization of facts is clearly a domain in which \glspl{llm} will continue to outperform humans.

% given that LLMs have parsed almost the entire internet, it is perhaps not surprising to seem them perform well on many of the task which often require 
% specific knowledge humans might not have at hand without consulting an additional knowledge bank 
% when it comes to reasoning tasks (NMR, point groups) the humans tend to beat LLMs 


% \subsection{Future work}
% - Advanced prompting techniques such as CoT and test time strategies

% - More advanced systems with chemnistry specific tools

% - More systematic human baseline: Ask people to come in, do not allow to skip questions

% clearly defined metrics important for deep learning https://towardsdatascience.com/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959

\section{Methods}

\subsection{Curation workflow}\label{sec:curation}
For our dataset we curated questions from existing exams, but also programmatically created new questions.
Questions were added via Pull Requests on our GitHub repository and only merged into the main collection after passing manual review.

To ensure that the questions do not enter a training dataset we use the canary string of the Big Bench project.
This requires that \Gls{llm} developers filter their training dataset for this canary string.

\begin{figure}
    \includegraphics[width = \textwidth]{figures/chem-bench.pdf}
    \caption{\textbf{Data curation workflow}.}
\end{figure}

\paragraph{Manually curated questions}

\subparagraph{Analytical chemistry}
\variable{output/question_count_per_dir/json_file_counts_analytical_chemistry.txt} questions are based on master student examination papers at the University of Jena (Germany). 
These questions focus on the principles and techniques employed in analytical chemistry.

\subparagraph{Combustion engineering}
\variable{output/question_count_per_dir/json_file_counts_combustion_engineering.txt} questions are based on master student examination papers at the University of Magdeburg (Germany). 
They explore topics related to combustion processes and engineering principles.

\subparagraph{Functional materials and nanomaterials}
\variable{output/question_count_per_dir/json_file_counts_func_mats_and_nanomats.txt}  questions are based on exercises in a seminar conducted at the University of Jena (Germany), delving into topics concerning functional materials and nanomaterials, exploring their properties and applications.

\subparagraph{General chemistry}
\variable{output/question_count_per_dir/json_file_counts_Gen_Chem_MCA.txt} questions originate from examinations for grade 10 at Marist Comprehensive Academy Uturu (Nigeria). 
They cover fundamental concepts in chemistry suitable for students at the secondary school level.
In addition, we included questions based on the general chemistry courses of the Bachelor of Science in Chemistry at the Technical University of Munich (Germany), focusing on inorganic chemistry (\variable{output/question_count_per_dir/json_file_counts_ac_faessler_tum.txt}) and principles of chemistry (\variable{output/question_count_per_dir/json_file_counts_pum_tum.txt}).

\subparagraph{International Chemical Olympiad}
\variable{output/question_count_per_dir/json_file_counts_icho.txt} questions are based on various countries' Chemical Olympiads, including those held in the USA, UK, and Moldova.

\subparagraph{Material synthesis}
\variable{output/question_count_per_dir/json_file_counts_materials_synthesis.txt} questions are based on seminars on material synthesis at the University of Jena (Germany), focusing on the methods and techniques employed in the synthesis of various materials.

\subparagraph{Organic reactivity}
\variable{output/question_count_per_dir/json_file_counts_organic_reactivity.txt}  questions are primarily sourced from tutorials and exam papers at for the Bachelor of Science in Chemistry at the Technical University of Munich (Germany), exploring reactions and mechanisms in organic chemistry.

\subparagraph{Periodic table}
\variable{output/question_count_per_dir/json_file_counts_periodic_table_properties.txt} questions are manually created to cover various aspects and trends of the periodic table.

\subparagraph{Polymer chemistry}
\variable{output/question_count_per_dir/json_file_counts_polymer_chemistry.txt} questions are based on examinations held at the University of Hamburg (Germany), covering topics related to polymer chemistry and its applications.

\subparagraph{Chemical reactivity}
\variable{output/question_count_per_dir/json_file_counts_reactive_groups.txt} questions are framed based on information from the Cameo Chemicals website (\url{https://cameochemicals.noaa.gov/reactivity}), exploring the reactivity of various chemicals and compounds.

\subparagraph{Textbook questions}
\variable{output/question_count_per_dir/json_file_counts_oup.txt} questions are based on questions from various textbooks covering biomolecular science, drug synthesis, molecular structure, organic chemistry, and X-ray crystallography.

\subparagraph{Chemical safety} 
This category comprises a total of \variable{output/question_count_per_dir/json_file_counts_safety.txt} questions, including semi-programmatically created questions covering areas such as GHS classification, hazard statements, \gls{dai}, and pictograms from the PubChem database \cite{pubchem} (see below for details). 
Additionally, it explores crucial topics like materials' compatibility (\variable{output/question_count_per_dir/json_file_counts_materials_compatibility.txt} questions) and chemical compatibility (\variable{output/question_count_per_dir/json_file_counts_chem_chem_comp.txt} questions), essential for understanding the hazardous impact of mixing certain materials and chemicals. 
Furthermore, it includes inquiries about the Federal/State Working Group on Chemical Safety (BLAC), consisting of \variable{output/question_count_per_dir/json_file_counts_blac_gfk.txt} questions. 
These questions are based on the question bank used for the expertise examination (\enquote{Sachkundepr√ºfung}) according to ¬ß11 of the Chemical Prohibition Ordinance (\enquote{Chemikalien-Verbotsverordnung}).
Moreover, this category addresses questions lab safety and topics such as pharmacology, toxicology, material safety data sheets, and chemical safety data sheets. 
The chemical safety category stands out as one of the primary focus areas within the \chembench corpus.

\subparagraph{NMR spectroscopy}
Questions are based on images shared via the Twitter account \url{https://twitter.com/NMRspectroscopy}, focusing choosing appropriate \gls{nmr} experiments.


\paragraph{Semi-programatically generated questions}

\subparagraph{Oxidation states}
\variable{output/question_count_per_dir/json_file_counts_oxidation_states.txt} questions regarding oxidation states are based on data from the website \url{https://www.cheminfo.org/}. 

\subparagraph{Total electron count of molecules}
\variable{output/question_count_per_dir/json_file_counts_electron_counts.txt} questions pertaining to the total electron count of molecules are based on data from the website \url{https://www.cheminfo.org/}. 
These inquiries involve determining the electron count of atoms in various chemical contexts. The calculations for these tasks are facilitated using the RDKit package \cite{rdkit}.

\subparagraph{Number of isomers}
We used MAYGEN\cite{Yirik_2021} to compute the number of isomers for a set of randomly sampled \gls{smiles} from the ZINC dataset.\cite{Irwin_2012}
To obtain the answers for false options we sampled from the set of number of isomers. In total, we generated \variable{output/question_count_per_dir/json_file_counts_number_of_isomers.txt} questions.

\subparagraph{Point group for molecules}
\variable{output/question_count_per_dir/json_file_counts_point_group.txt} questions involve predicting the point group for molecules. Our ChemCaption tool (\url{https://github.com/lamalab-org/chem-caption}) is utilized for data generation. 
Given a molecule, it uses spglib\cite{spglib} to identify the point group. 
Assigned point groups were manually verified to only include well-defined cases.

\subparagraph{SMILES-IUPAC}
We scraped pairs of \gls{smiles} and \gls{iupac} names from the PubChem database \cite{pubchem} and then framed \variable{output/question_count_per_dir/json_file_counts_smiles_to_name.txt} questions involve converting \gls{smiles} notation to \gls{iupac} nomenclature.
\variable{output/question_count_per_dir/json_file_counts_name_to_smiles.txt} questions involve the reverse task, i.e., converting \gls{iupac} nomenclature to \gls{smiles} notation.


\subparagraph{Number of NMR peaks} 
To generate the \variable{output/question_count_per_dir/json_file_counts_name_to_smiles.txt}  tasks about the number of \gls{nmr} peaks, we randomly sampled \gls{smiles} from the ZINC database\cite{Irwin_2012} and then used OpenChemLib\cite{openchemlib} to compute the number of diasterotopically distinct hydrogen atoms. 
We then sampled from the set of number of peaks (excluding the correct answer) to obtain the false answer options.

\subparagraph{GHS, hazard statements and DAI}
The \gls{ghs} classification, hazard statements and \gls{dai} data have been extracted from PubChem.\cite{pubchem}
The former two have been mined via the PubChem \gls{api}, while the latter has been manually compiled. 
In total, we generated \variable{output/question_count_per_dir/json_file_counts_pictograms.txt} questions about the \gls{ghs} classification of chemicals, \variable{output/question_count_per_dir/json_file_counts_h_statements.txt} questions about definition of hazard statements, and \variable{output/question_count_per_dir/json_file_counts_dai.txt} questions about \glspl{dai}.
The \glspl{dai} have been curated to contain only records approved by the \gls{who}.
The chemicals in this class of questions belong to one of the three classes: pesticides (e.g., calcium arsenate), insecticides (e.g., cyfluthrin) or herbicides (e.g. 2,4-D).


\subsection{Model evaluation workflow}

\paragraph{Prompting}

To maintain the consistency with the training, we employ distinct prompt templates in general tailored for completion models and instruction-tuned models. 
We pose instruction or impose constraints on the models within these templates to receive response in a specific format so that a robust, fair and consistent parsing can be performed as explained in \Cref{sec:parsing}.
Certain models are trained with special annotations and LaTeX syntax for scientific notations, chemical reactions, or symbols embedded within the text. 
For example, all the SMILES representations are encapsulated within \texttt{[START\_SMILES][\text backslash END\_SMILES]} in Galactica\cite{taylor2022galactica}.
Our prompting strategy consistently adheres to  these details in a model-specific manner by post-processing \LaTeX syntax, chemical symbols, chemical equations and physical units (by either adding or removing wrappers).



\paragraph{Parsing}
Our parsing workflow is, by default, multistep and primarily based on regular expressions.
In the case of instruction-tuned models we first attempt to identify the \texttt{[ANSWER][\textbackslash ANSWER]} environment we prompt the model to report the answer in.
In the case of completion models, this step is skipped. From there, we attempt to extract the relevant enumeration letters (for multiple-choice questions) or numbers.
In the case of numbers, our regular expression was engineered to be able to deal with various forms of scientific notation.
As initial tests indicated that models sometimes return integers in the form of words, e.g. \enquote{one} instead of \enquote{1}, we also implemented a word-to-number conversion.
In case these hard-coded parsing steps fail, we fall back to using a \gls{llm}, e.g. Claude-2, to parse the completion.
Manual verification indicates that this \gls{llm}-based parsing is not a relevant error source and has been performed correctly in all cases we checked (randomly sampled 4 questions per topic).

\paragraph{Models}
\subparagraph{Completion models}
We used \texttt{Galactica-120b}\cite{taylor2022galactica} with the default settings.


\subparagraph{Instruction-tuned models} In addition, we used \texttt{Claude 2}, \texttt{Claude3 (Opus)},\cite{anthropicClaudeModelFamily2024} \texttt{GPT-4},\cite{openai2024gpt4} \texttt{GPT-3.5-turbo},\cite{brown2020language}
\texttt{Gemini Pro},\cite{gemini}
\texttt{Mixtral-8x7b}\cite{jiang2024mixtral}
\texttt{Llama2-70b},\cite{touvron2023llama}
\texttt{pplx-7b-chat}

\subparagraph{Tool augmented models}
In addition to directly prompting \glspl{llm}, we also investigated the performance of tool-augmented systems.
For this we, on the one hand, investigated the \texttt{online} model of Perplexity.AI and, on the other hand, \texttt{gpt-3.5-turbo} as well as \texttt{claude-2}, with ReAct-style tool augmentation.\cite{yao2023react}
The latter two models had access to WolframAlpha, the ArXiv \gls{api}, a Python interpreter, as well as web search (using DuckDuckGo).
We implemented the system using Langchain with the default prompts and constrained the system to a maximum of ten \gls{llm} calls.


\subsection{Confidence estimate}
To estimate the models' confidence we prompted them with the question (and answer options for \gls{mcq}) and the task to rate their confidence to produce the right answer on a scale from 1 to 5. 

\subsection{Human baseline}

\paragraph{App} To facilitate the collection of responses, we developed a responsive web application in Typescript using the Next.js\cite{nextjs} app router framework.
This application handles serving the user interface as well as exposes various \gls{rest} \glspl{api} for relevant operations.
We utilize a MySQL\cite{mysql} database and Prisma \gls{orm}\cite{prisma} for efficient database management.
The web application is styled with Tailwind CSS\cite{tailwindcss} using the shadcn/ui component library and uses NextAuth\cite{nextauth} for easy and secure user authentication and postMark for sending Emails.
The application is hosted on the Vercel web hosting platform.

\paragraph{Question selection} \label{sec:subset-selection}
Since we anticipated that we will not be able to collect enough responses for every question to allow for a meaningful statistical analysis, we decided on showing a relevant subset of all questions to the human scorers.
For selecting the subset, we decided on addressing two questions:
\begin{itemize}
    \item Are the questions for which the models scored poorly just too difficult or unanswerable?
    \item Are there areas in which the performance of humans is very different from the ones of the models?
\end{itemize}
To answer the first question we selected X questions that all \glspl{llm} (model names) from an initial scoring round did not answer correctly.
From those we picked X diverse one using greedy MaxMin sampling on the embeddings on the questions computed using BART (see below).


\paragraph{Study design}
For our initial study we wanted to maximize the response rate given our available resources. 
For this reason, we did not opt for a highly controlled study setting. 
That is, while users were prompted to not use external tools other than a calculator and to not consult with other humans, we do not have any way to verify that the participants complied with those rules. 
Note that users were also allowed to skip questions.

Another aspect of requiring unsupervised question answering is that in real life humans have tools and are able to use them for answering any question of interest. 
In our current study, we prompted users to not use those tools.

\paragraph{Participants}
Users were open to report about their experience in chemistry. 
Overall, \variable{output/num_users_with_education_info.txt} did so. 
Out of those, \variable{output/num_human_phd.txt} reported to have been awarded a Ph.D.
\variable{output/num_human_postdoc.txt} are beyond a first postdoc, \variable{output/num_human_master.txt} have a master's degree, and \variable{output/num_human_bachelor.txt} have a bachelor's degree and for \variable{output/num_human_highschool.txt} the highest awarded degree is from high school.


\paragraph{Comparison with models}
To compare the performance of humans (who might have answered only some questions) with the performance of models (which answered all questions), we focussed on questions which at least four humans answered and limited the pool of human scorers to those who answered at least 100 questions (i.e., \variable{output/num_humans_with_more_than_100_scores.txt} humans). 
The latter threshold was chosen to limit it to humans who made serious attempts at systematically answering a part of the questions. 
This analysis might lead to potential biases, most likely in favor of humans as they were allowed to skip questions. \variable{output/num_humans_with_204_scores.txt} humans answered more than 200 questions.


\subsection{Classification of questions into topics}\label{sec:meth-topic} When curating our dataset we systematically recorded keywords and sources.
To allow for analysis of the model performance as a function of the topic, we leverage this information together with sequence classification models.
For questions which can easily be assigned to a topic based on the source (e.g., number of NMR peaks, chemical compatibility, toxicology exam questions) we use this information to make the assignment.
For the remaining ones, e.g., from chemistry olympiad questions, we use zero-short sequence classification\cite{zeroshotsequence} using the BART model\cite{bart, FacebookBART}, which our preliminary analysis found to be more robust than topic modeling based on embeddings from OpenAI's \texttt{ada} model or Cohere's \texttt{Cohembed-english-v3.0} model.


\section*{Data and code availability}
The code and data for \chembench is available at \url{https://github.com/lamalab-org/chem-bench} and archived at \url{XXX}.
The code for the app for our human baseline study is available at \url{https://github.com/lamalab-org/chem-bench-app}. 
To ensure reproducibility, this manuscript was generated using the \href{https://show-your.work/en/latest/}{showyourwork!} framework.\cite{Luger2021}
The code to rebuild the paper (including code for all figures and numbers next to which there is a GitHub icon) can be found at \url{https://github.com/lamalab-org/chembench-paper}. 
To facilitate reproduction, some intermediate results of the analysis are cached at \url{http://dx.doi.org/10.5072/zenodo.34706}.

\section*{Acknowledgements}
This work was supported by the Carl Zeiss Foundation and a \enquote{Talent Fund} of the \enquote{Life} profile line of the Friedrich Schiller University Jena.
We also want to acknowledge access to the HPC cluster of Stability.AI.
M.A. expresses gratitude to the European Research Council (ERC) for evaluating the project with the reference number 101106377 titled ‚Äò‚ÄòCLARIFIER‚Äô‚Äô and accepting it for funding under the HORIZON TMA MSCA Postdoctoral Fellowships - European Fellowships. 
Furthermore, M.A. acknowledges the funding provided by UK Research and Innovation (UKRI) under the UK government‚Äôs Horizon Europe funding guarantee (Grant Reference: EP/Y023447/1; Organization Reference: 101106377).

The authors thank Julian Kimmig for feedback on the web app. 

\section*{Conflicts of interest}
K.M.J.\ is a paid consultant for OpenAI. M.P.\ is employee of Stability.ai and A.M.\ and N.A.\ are paid contractors of Stability.AI.

\section*{Author contributions}

\footnotesize
\insertcredits
\normalsize



\appendix
\section{Appendix}
\input{appendix.tex}

\bibliography{references}

\end{document}
